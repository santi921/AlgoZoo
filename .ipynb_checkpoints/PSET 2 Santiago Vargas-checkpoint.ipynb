{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from sklearn import linear_model\n",
    "import scipy.optimize as spopt\n",
    "import sklearn\n",
    "\n",
    "from __future__ import division\n",
    "from numpy.random import rand\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "#change cache size for svm\n",
    "#svd\n",
    "\n",
    "#Setting C: C is 1 by default and itâ€™s a reasonable default choice. If you have a lot of noisy observations you should decrease it. It corresponds to regularize more the estimation.\n",
    "#idea: use a gradient-descent method to optimize c for the optimization bit\n",
    "\n",
    "\n",
    "#Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. \n",
    "#Model with high bias pays very little attention to the training data and oversimplifies the model.\n",
    "#It always leads to high error on training and test data.\n",
    "\n",
    "#Variance is the variability of model prediction for a given data point or a value which tells us spread of our data\n",
    "\n",
    "#In short, variance is a the tendancy to overfit and bias is the tendancy to oversimply data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week you did the tensor\n",
    "ow MNIST tutorial to classify images. Mod-\n",
    "ify this code to carry out neural network classi\f",
    "cation on this dataset.\n",
    "Report your results on Kaggle. Whoever \f",
    "nds a modi\f",
    "cation of the neu-\n",
    "ral network to perform best on the dataset wins!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "highT = []\n",
    "for i in range(500):\n",
    "    highT.append(np.loadtxt('./PS2-a-highT/'+str(i).zfill(3), delimiter=',')) \n",
    "\n",
    "lowT = []\n",
    "for i in range(500):\n",
    "    lowT.append(np.loadtxt('./PS2-a-lowT/'+str(i).zfill(3), delimiter=',')) ## Change to your local directory!\n",
    "\n",
    "dataImg = np.concatenate((highT,lowT))\n",
    "#dataImg = dataImg.reshape((-1, 64*64))\n",
    "dataLabel = np.concatenate((np.ones(len(highT)), np.zeros(len(lowT))))\n",
    "print(dataImg.shape)\n",
    "\n",
    "trainImg, testImg, trainLabel, testLabel = train_test_split(\n",
    "    dataImg, dataLabel, test_size=1/5.0, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batchTrial(batch, labels, neuronA, neuronB, dropPA, dropPB, epoch ):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(64, 64)))\n",
    "    model.add(tf.keras.layers.Dense(neuronA, kernel_initializer='normal' , activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dropout(dropPA))\n",
    "    model.add(tf.keras.layers.Dense(neuronB, kernel_initializer='normal' , activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dropout(dropPB))\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer='adam',  loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(batch, labels, epochs=epoch,verbose=0)\n",
    "    score = model.evaluate(testImg, testLabel)\n",
    "    print(\"batch size: {} score:{}\".format(batch.size,score[1]))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 35s 177ms/step\n",
      "batch size: 819200 score:0.68\n",
      "0.68\n",
      "0 0.68\n",
      "200/200 [==============================] - 26s 130ms/step\n",
      "batch size: 819200 score:0.635\n",
      "0.635\n",
      "200/200 [==============================] - 30s 149ms/step\n",
      "batch size: 819200 score:0.67\n",
      "0.67\n",
      "200/200 [==============================] - 30s 149ms/step\n",
      "batch size: 819200 score:0.7\n",
      "0.7\n",
      "3 0.7\n",
      "200/200 [==============================] - 30s 151ms/step\n",
      "batch size: 819200 score:0.63\n",
      "0.63\n",
      "200/200 [==============================] - 30s 148ms/step\n",
      "batch size: 819200 score:0.65\n",
      "0.65\n",
      "200/200 [==============================] - 31s 153ms/step\n",
      "batch size: 819200 score:0.605\n",
      "0.605\n",
      "200/200 [==============================] - 30s 149ms/step\n",
      "batch size: 819200 score:0.74\n",
      "0.74\n",
      "7 0.74\n",
      "200/200 [==============================] - 32s 158ms/step\n",
      "batch size: 819200 score:0.675\n",
      "0.675\n",
      "200/200 [==============================] - 31s 154ms/step\n",
      "batch size: 819200 score:0.585\n",
      "0.585\n",
      "200/200 [==============================] - 32s 160ms/step\n",
      "batch size: 819200 score:0.685\n",
      "0.685\n",
      "200/200 [==============================] - 31s 157ms/step\n",
      "batch size: 819200 score:0.63\n",
      "0.63\n",
      "200/200 [==============================] - 32s 160ms/step\n",
      "batch size: 819200 score:0.675\n",
      "0.675\n",
      "200/200 [==============================] - 33s 163ms/step\n",
      "batch size: 819200 score:0.67\n",
      "0.67\n",
      "200/200 [==============================] - 32s 161ms/step\n",
      "batch size: 819200 score:0.625\n",
      "0.625\n",
      "200/200 [==============================] - 32s 162ms/step\n",
      "batch size: 819200 score:0.62\n",
      "0.62\n",
      "200/200 [==============================] - 33s 164ms/step\n",
      "batch size: 819200 score:0.67\n",
      "0.67\n",
      "200/200 [==============================] - 32s 162ms/step\n",
      "batch size: 819200 score:0.62\n",
      "0.62\n",
      "200/200 [==============================] - 32s 160ms/step\n",
      "batch size: 819200 score:0.685\n",
      "0.685\n",
      "200/200 [==============================] - 33s 167ms/step\n",
      "batch size: 819200 score:0.575\n",
      "0.575\n",
      "200/200 [==============================] - 33s 167ms/step\n",
      "batch size: 819200 score:0.665\n",
      "0.665\n",
      "200/200 [==============================] - 34s 169ms/step\n",
      "batch size: 819200 score:0.68\n",
      "0.68\n",
      "200/200 [==============================] - 34s 170ms/step\n",
      "batch size: 819200 score:0.65\n",
      "0.65\n",
      "200/200 [==============================] - 34s 171ms/step\n",
      "batch size: 819200 score:0.67\n",
      "0.67\n",
      "200/200 [==============================] - 34s 172ms/step\n",
      "batch size: 819200 score:0.645\n",
      "0.645\n",
      "200/200 [==============================] - 35s 173ms/step\n",
      "batch size: 819200 score:0.68\n",
      "0.68\n",
      "200/200 [==============================] - 35s 177ms/step\n",
      "batch size: 819200 score:0.69\n",
      "0.69\n",
      "200/200 [==============================] - 35s 176ms/step\n",
      "batch size: 819200 score:0.66\n",
      "0.66\n",
      "200/200 [==============================] - 36s 182ms/step\n",
      "batch size: 819200 score:0.55\n",
      "0.55\n",
      "200/200 [==============================] - 35s 173ms/step\n",
      "batch size: 819200 score:0.575\n",
      "0.575\n",
      "200/200 [==============================] - 37s 183ms/step\n",
      "batch size: 819200 score:0.665\n",
      "0.665\n",
      "200/200 [==============================] - 37s 184ms/step\n",
      "batch size: 819200 score:0.6\n",
      "0.6\n",
      "200/200 [==============================] - 36s 182ms/step\n",
      "batch size: 819200 score:0.64\n",
      "0.64\n",
      "200/200 [==============================] - 38s 192ms/step\n",
      "batch size: 819200 score:0.645\n",
      "0.645\n",
      "200/200 [==============================] - 38s 188ms/step\n",
      "batch size: 819200 score:0.635\n",
      "0.635\n",
      "200/200 [==============================] - 38s 188ms/step\n",
      "batch size: 819200 score:0.6\n",
      "0.6\n",
      "200/200 [==============================] - 38s 189ms/step\n",
      "batch size: 819200 score:0.69\n",
      "0.69\n",
      "200/200 [==============================] - 38s 190ms/step\n",
      "batch size: 819200 score:0.685\n",
      "0.685\n",
      "200/200 [==============================] - 38s 192ms/step\n",
      "batch size: 819200 score:0.64\n",
      "0.64\n",
      "200/200 [==============================] - 38s 192ms/step\n",
      "batch size: 819200 score:0.73\n",
      "0.73\n",
      "200/200 [==============================] - 39s 193ms/step\n",
      "batch size: 819200 score:0.75\n",
      "0.75\n",
      "40 0.75\n",
      "200/200 [==============================] - 38s 190ms/step\n",
      "batch size: 819200 score:0.685\n",
      "0.685\n",
      "200/200 [==============================] - 39s 196ms/step\n",
      "batch size: 819200 score:0.615\n",
      "0.615\n",
      "200/200 [==============================] - 39s 194ms/step\n",
      "batch size: 819200 score:0.645\n",
      "0.645\n",
      "200/200 [==============================] - 41s 203ms/step\n",
      "batch size: 819200 score:0.585\n",
      "0.585\n",
      "200/200 [==============================] - 40s 202ms/step\n",
      "batch size: 819200 score:0.645\n",
      "0.645\n",
      "200/200 [==============================] - 42s 208ms/step\n",
      "batch size: 819200 score:0.71\n",
      "0.71\n",
      "200/200 [==============================] - 41s 204ms/step\n",
      "batch size: 819200 score:0.59\n",
      "0.59\n",
      "200/200 [==============================] - 41s 207ms/step\n",
      "batch size: 819200 score:0.625\n",
      "0.625\n",
      "200/200 [==============================] - 41s 207ms/step\n",
      "batch size: 819200 score:0.725\n",
      "0.725\n"
     ]
    }
   ],
   "source": [
    "#Idea: Batch Hyperparameter Optimization\n",
    "#1: randomly sample a subset of data\n",
    "#2: optimize over these parameters in a bounded sense, randomly accessing these parameters\n",
    "#3: store the value of the loses and accuracy\n",
    "#4: extract max \n",
    "\n",
    "bound = len(trainImg)\n",
    "batchSize = 200\n",
    "trials = 50\n",
    "acc_best = 0.5\n",
    "best_para = np.zeros(5)\n",
    "\n",
    "for i in range(trials):\n",
    "    batchInd = random.sample(range(0,bound), batchSize)\n",
    "    \n",
    "    batch = trainImg[batchInd]\n",
    "    batchLabels = trainLabel[batchInd]\n",
    "    \n",
    "    neuronA = random.sample(range(1,2048),1)[0]\n",
    "    neuronB = random.sample(range(1,1028),1)[0]\n",
    "    dropPA = random.random()/2.\n",
    "    dropPB = random.random()/2.\n",
    "    epoch = 5\n",
    "    \n",
    "    \n",
    "    loss, acc= batchTrial(batch, batchLabels, neuronA, neuronB, dropPA, dropPB,epoch)\n",
    "    print(acc)\n",
    "    if (acc > acc_best):\n",
    "        print(i, acc)\n",
    "        acc_best = acc\n",
    "        best_para = np.array([neuronA, neuronB, dropPA, dropPB,epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.83600000e+03 9.27000000e+02 2.74282660e-01 7.03496768e-02]\n",
      "Epoch 1/12\n",
      "800/800 [==============================] - 12s 15ms/step - loss: 4.7459 - acc: 0.5813\n",
      "Epoch 2/12\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 2.0819 - acc: 0.7937\n",
      "Epoch 3/12\n",
      "800/800 [==============================] - 5s 7ms/step - loss: 0.6784 - acc: 0.8938\n",
      "Epoch 4/12\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.1967 - acc: 0.9650\n",
      "Epoch 5/12\n",
      "800/800 [==============================] - 5s 7ms/step - loss: 0.1015 - acc: 0.9788\n",
      "Epoch 6/12\n",
      "800/800 [==============================] - 5s 7ms/step - loss: 0.0207 - acc: 0.9925\n",
      "Epoch 7/12\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.0119 - acc: 0.9962\n",
      "Epoch 8/12\n",
      "800/800 [==============================] - 5s 7ms/step - loss: 0.0098 - acc: 0.9950\n",
      "Epoch 9/12\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 10/12\n",
      "800/800 [==============================] - 5s 7ms/step - loss: 0.0024 - acc: 0.9988\n",
      "Epoch 11/12\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 5.9116e-04 - acc: 1.0000\n",
      "Epoch 12/12\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 2.1771e-04 - acc: 1.0000\n",
      "200/200 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7808729648590088, 0.83]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract best values from batch\n",
    "\n",
    "best_para = np.array([1.83600000e+03, 9.27000000e+02, 2.74282660e-01, 7.03496768e-02])\n",
    "print(best_para)\n",
    "\n",
    "neuronA, neuronB, dropPA, dropPB= best_para[0],best_para[1],best_para[2],best_para[3]\n",
    "epoch = 12\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(64, 64)))\n",
    "model.add(tf.keras.layers.Dense(neuronA, kernel_initializer='normal' , activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dropout(dropPA))\n",
    "model.add(tf.keras.layers.Dense(neuronB, kernel_initializer='normal' , activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dropout(dropPB))\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer='adam',  loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "model.fit(trainImg, trainLabel, epochs=epoch,verbose=1)\n",
    "model.evaluate(testImg, testLabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "1.0\n",
      "0.83\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(testImg)\n",
    "print(predict.size)\n",
    "print(testImg[1][1][1])\n",
    "temp = 0 \n",
    "for i,pred in enumerate(predict):\n",
    "    if np.around(pred) == testLabel[i]:\n",
    "        temp+=1\n",
    "        \n",
    "print(temp/testLabel.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "hwa = []\n",
    "with open(\"C:/Users/Dell/Google Drive/Repos/AM 216/PSET 2/hw2p1a_testset.csv\") as foo:\n",
    "    for line in csv.reader(foo):\n",
    "        hwa.append(np.array(line).reshape(64,64))\n",
    "        \n",
    "\n",
    "testImgHW = np.array(hwa)\n",
    "print(testImgHW.shape)\n",
    "#testImgHW = testImgHW.reshape((-1, 64*64))\n",
    "\n",
    "predictHWA = model.predict(testImgHW)\n",
    "predictHWA = np.around(predictHWA)\n",
    "np.savetxt(\"predictAraw.csv\", predictHWA, delimiter=\",\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a SVM (SVC) to train the classi\f",
    "cation model. Report your results\n",
    "on Kaggle. Pay attention to the kernel you used and its parameters.\n",
    "These will a\u000b",
    "ect the accuracy of your classi\f",
    "er."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Part A. I tried first a package i found for hyperparamter optimization but I think its was not sensitive enough for the regime of hyperparameters we needed for worthwhile data in this problems. I then did a simple coarse-grain grid search followed by a finer grain to optimize for the best parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm, metrics\n",
    "#kernel from sklearn import datasets, svm, metrics\n",
    "import optunity\n",
    "import optunity.metrics\n",
    "\n",
    "#This is a hyperparameter optimization package i found but didn't work for my data. Seemed worthwhile to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n",
      "yeet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "@optunity.cross_validated(x=data_img, y=data_label, num_folds=10, num_iter=2)\n",
    "\n",
    "def svm_auc(x_train, y_train, x_test, y_test, logC, logGamma):\n",
    "    model = sklearn.svm.SVC(C=10 ** logC, gamma=10 ** logGamma).fit(x_train, y_train)\n",
    "    decision_values = model.decision_function(x_test)\n",
    "    print(\"yeet\")\n",
    "    return optunity.metrics.roc_auc(y_test, decision_values)\n",
    "\n",
    "\n",
    "hps, _, _ = optunity.maximize(svm_auc, num_evals=2, logC=[195, 200], logGamma=[-5, -4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution\n",
      "========\n",
      "logC \t 195.60546875\n",
      "logGamma \t -4.12109375\n"
     ]
    }
   ],
   "source": [
    "solution = dict([(k, v) for k, v in hps.items() if v is not None])\n",
    "print('Solution\\n========')\n",
    "print(\"\\n\".join(map(lambda x: \"%s \\t %s\" % (x[0], str(x[1])), solution.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=4.0315193628604397e+195, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=7.566695371401163e-05,\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "[0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 1. 0. 1.]\n",
      "0.89\n"
     ]
    }
   ],
   "source": [
    "optimal_model = sklearn.svm.SVC(C=10 ** hps['logC'], gamma=10 ** hps['logGamma']).fit(train_img, train_lbl)\n",
    "print(optimal_model)\n",
    "predicted = optimal_model.predict(test_img)\n",
    "print(predicted)\n",
    "\n",
    "temp = 0\n",
    "for i,pred in enumerate(predicted):\n",
    "            \n",
    "    if (pred == test_lbl[i]):\n",
    "\n",
    "        temp+=1\n",
    "\n",
    "power = temp/test_lbl.size\n",
    "\n",
    "print(power)\n",
    "\n",
    "#printing the prediction array you can see that this did not work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([\n",
    " 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1. ,1., 1.,\n",
    " 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
    " 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
    " 0., 1., 0., 0., 1., 0., 0., 0., 1. ,1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
    " 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
    " 0., 0., 1., 0., 0., 0., 1., 1., 0. ,0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
    " 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    " 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
    " 1., 1., 0., 0., 0., 1., 0., 1.])\n",
    "np.savetxt(\"predictA.csv\", a, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-34ebc0d017de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlow_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mlow_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Dell/Google Drive/Repos/AM 216/PSET 2/PS2-a-lowT/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding)\u001b[0m\n\u001b[0;32m   1090\u001b[0m         \u001b[1;31m# converting the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1092\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(chunk_size)\u001b[0m\n\u001b[0;32m   1006\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfirst_line\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1008\u001b[1;33m             \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1009\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "high_t = []\n",
    "for i in range(500):\n",
    "    high_t.append(np.loadtxt('C:/Users/Dell/Google Drive/Repos/AM 216/PSET 2/PS2-a-highT/'+str(i).zfill(3), delimiter=',')) \n",
    "\n",
    "low_t = []\n",
    "for i in range(500):\n",
    "    low_t.append(np.loadtxt('C:/Users/Dell/Google Drive/Repos/AM 216/PSET 2/PS2-a-lowT/'+str(i).zfill(3), delimiter=','))\n",
    "\n",
    "    \n",
    "data_img = np.concatenate((high_t,low_t))\n",
    "data_img = data_img.reshape((-1, 64*64))\n",
    "data_label = np.concatenate((np.ones(len(high_t)), np.zeros(len(low_t))))\n",
    "\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_img, test_img, train_lbl, test_lbl = train_test_split(data_img, data_label, test_size=1/5.0, random_state=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88\n",
      "C:2.0000000000000002e+197 g: 7.187627327609251e-05 accuracy:0.88\n",
      "0.91\n",
      "C:3.98e+199 g: 0.00010382128362102251 accuracy:0.91\n",
      "0.945\n",
      "C:3.98e+199 g: 0.00013576629396595252 accuracy:0.945\n",
      "0.945\n",
      "0.95\n",
      "C:3.98e+199 g: 0.00019965631465581254 accuracy:0.95\n",
      "0.955\n",
      "C:3.98e+199 g: 0.00023160132500074255 accuracy:0.955\n",
      "0.955\n",
      "0.945\n",
      "0.945\n",
      "0.945\n"
     ]
    }
   ],
   "source": [
    "best =0.5\n",
    "bestC =0\n",
    "bestJ =0\n",
    "#experimentally found through previous grid searches\n",
    "scaleC = 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
    "\n",
    "scaleG = 0.00035938136638046257\n",
    "\n",
    "for i in range(1,3,10):\n",
    "    for j in np.linspace(1,5,10):\n",
    "        \n",
    "        cTemp =scaleC * (i/5)  \n",
    "        gTemp =scaleG * (j/5)\n",
    "        \n",
    "        classifier = svm.SVC(C=cTemp,kernel='rbf',gamma=gTemp,cache_size=8000,probability=False)\n",
    "        classifier.fit(train_img, train_lbl)\n",
    "        predicted = classifier.predict(test_img)\n",
    "        \n",
    "        temp = 0 \n",
    "        for i,pred in enumerate(predicted):\n",
    "            \n",
    "            if (pred == test_lbl[i]):\n",
    "            \n",
    "                temp+=1\n",
    "                \n",
    "        power = temp/test_lbl.size\n",
    "        \n",
    "        print(power)\n",
    "        \n",
    "        if (power > best):\n",
    "            \n",
    "            best = power\n",
    "            bestC = cTemp \n",
    "            bestJ = gTemp\n",
    "            \n",
    "            print(\"C:{} g: {} accuracy:{}\".format(cTemp,gTemp,power))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 1. 0. 1.]\n",
      "0.955\n"
     ]
    }
   ],
   "source": [
    "cTemp = bestC\n",
    "gTemp = bestJ\n",
    "classifier = svm.SVC(C=cTemp,kernel='rbf',gamma=gTemp,cache_size=8000,probability=False)\n",
    "classifier.fit(train_img, train_lbl)\n",
    "predicted = classifier.predict(test_img)\n",
    "print(predicted)\n",
    "print(best)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "hwb = []\n",
    "with open(\"C:/Users/Dell/Google Drive/Repos/AM 216/PSET 2/hw2p1b_testset.csv\") as foo:\n",
    "    for line in csv.reader(foo):\n",
    "        hwb.append(np.array(line).reshape(64,64))\n",
    "        \n",
    "\n",
    "testImgHW = np.array(hwb)\n",
    "testImgHW = testImgHW.reshape((-1, 64*64))\n",
    "\n",
    "predictHWB = classifier.predict(testImgHW)\n",
    "np.savetxt(\"predictBraw.csv\", predictHWB, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried two methods to tune the parameters. For both I started with a course-grained grid of values to get a sense of where my model would roughly become sensitive to the data. From there I tried using a package I found online called optunity. This yieled a best result of 0.89 accuracy. I also tried a more fine-grained grid in the are around where my model was more effective and I was able to boost my model from 0.9 to 0.955 accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===============C===============C===============C===============C===============C===============C===============\n",
    "\n",
    "If you successfully make your SVM work, please explain why and how it\n",
    "can separate these two phases under your chosen Kernel and parameters.\n",
    "Please use quantitative evidence to support your argument. (Bonus:) We\n",
    "saw in class that Logistic regression doesn't work so well. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters used were: \n",
    "\n",
    "rbf with C:3.98e+199 g: 0.00023160132500074255\n",
    "\n",
    "Here the rbf kernel was the most effective owing to the fact that there were no clear, dividing hyper planes between the Ising temperature data. This meant that it was in the regime for the rbf to be effective. \n",
    "\n",
    "Given how high the value of C is for this data it means that the model ended up training to a huge degree in order to not misclassify training data and thus creating small \"pockets\" in 2D space of representation. The higher value of C allows for boundaries that are tighter against data boundaries \n",
    "\n",
    "The small value for gamma meant that the model attempted to reduce the curvature of the graphs in order to be more generalizable \n",
    "source: https://chrisalbon.com/machine_learning/support_vector_machines/svc_parameters_using_rbf_kernel/\n",
    "\n",
    "Logistic Regression is not too effective in this example because the data is not arranged in a manner than is easily separable by a line or plane. In addition, I predict that this data is highly correlated and thus the logistic regression model performs poorly under these regimes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D====================D====================D====================D====================D====================\n",
    "\n",
    "For this problem use the same two 32\u000232 datasets that we used in the lab\n",
    "to save you time generating your own data. We have uploaded the data as\n",
    "`PS2-d-highT' and `PS2-d-lowT' on canvas. Train a model based on these\n",
    "two datasets, as in the problems above. Now generate 32 \u0002 32 samples\n",
    "from several di\u000b",
    "erent temperatures using the Ising() class provided in lab\n",
    "(you can modify the codes according to your preference, if you want).\n",
    "Use the trained model to somehow estimate the transition temperature.\n",
    "Enter your estimated transition temperature in Kaggle. [Hint: What is\n",
    "the physical meaning of a state that is on the decision boundary of SVM?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#problem: we want ot create a metric of transition temperature\n",
    "#\n",
    "import numpy\n",
    "import pandas\n",
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating the Ising model\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Ising():\n",
    "    ''' Simulating the Ising model '''  \n",
    "    def __init__(self, size, temp):\n",
    "        self.temp = temp\n",
    "        self.N = int(size)\n",
    "    ## monte carlo moves\n",
    "    def mcmove(self, config, N, beta):\n",
    "        ''' This is to execute the monte carlo moves using \n",
    "        Metropolis algorithm such that detailed\n",
    "        balance condition is satisified'''\n",
    "        for i in range(N):\n",
    "            for j in range(N):            \n",
    "                    a = np.random.randint(0, N) # select a row\n",
    "                    b = np.random.randint(0, N) # select a column\n",
    "                    s =  config[a, b] # current state at (a, b)\n",
    "                    # periodic boundary condition imposed\n",
    "                    nb = config[(a+1)%N,b] + config[a,(b+1)%N] + config[(a-1)%N,b] + config[a,(b-1)%N]\n",
    "                    cost = 2*s*nb\n",
    "                    if cost < 0:\n",
    "                        s *= -1\n",
    "                    elif rand() < np.exp(-cost*beta):\n",
    "                        s *= -1\n",
    "                    config[a, b] = s\n",
    "        return config\n",
    "    \n",
    "    def simulate(self):   \n",
    "        ''' This module simulates the Ising model'''\n",
    "        config = 2*np.random.randint(2, size=(self.N,self.N))-1   \n",
    "        times = 100\n",
    "        for i in range(times):\n",
    "            self.mcmove(config, self.N, 1.0/self.temp)\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1024)\n",
      "(200, 1024)\n",
      "(800, 1024)\n"
     ]
    }
   ],
   "source": [
    "highT = []\n",
    "for i in range(500):\n",
    "    highT.append(np.loadtxt('./PS2-d-highT/'+str(i).zfill(3), delimiter=',')) \n",
    "\n",
    "lowT = []\n",
    "for i in range(500):\n",
    "    lowT.append(np.loadtxt('./PS2-d-lowT/'+str(i).zfill(3), delimiter=',')) ## Change to your local directory!\n",
    "\n",
    "dataImg = np.concatenate((highT,lowT))\n",
    "#dataImg = dataImg.reshape((-1, 32*32))\n",
    "dataLabel = np.concatenate((np.ones(len(highT)), np.zeros(len(lowT))))\n",
    "print(dataImg.shape)\n",
    "\n",
    "train_img, test_img, train_lbl, test_lbl = train_test_split(dataImg, dataLabel, test_size=1/5.0, random_state=0)\n",
    "print(test_img.shape)\n",
    "print(train_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 1. 0. 1.]\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "#I will used SVM since that had the best results for the preceding model. I will try with the same parameters to see if I get similar valves for accuracy \n",
    "\n",
    "\n",
    "#Step 1: build predictor for 32 by 32 space\n",
    "\n",
    "cTemp = bestC\n",
    "gTemp = bestJ\n",
    "classifierD = svm.SVC(C=cTemp,kernel='rbf',gamma=gTemp,cache_size=8000,probability=False)\n",
    "classifierD.fit(train_img, train_lbl)\n",
    "predictedIsing = classifierD.predict(test_img)\n",
    "print(predictedIsing)\n",
    "\n",
    "\n",
    "temp = 0\n",
    "for i,pred in enumerate(predictedIsing):\n",
    "    if (pred == test_lbl[i]):\n",
    "        temp+=1\n",
    "power = temp/test_lbl.size\n",
    "print(power)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1 -1 -1 ... -1 -1 -1]\n",
      "  [-1 -1 -1 ... -1 -1 -1]\n",
      "  [-1 -1 -1 ... -1 -1 -1]\n",
      "  ...\n",
      "  [-1 -1 -1 ... -1 -1 -1]\n",
      "  [-1 -1 -1 ... -1 -1 -1]\n",
      "  [-1 -1 -1 ... -1 -1 -1]]\n",
      "\n",
      " [[ 1  1  1 ...  1  1  1]\n",
      "  [ 1  1  1 ...  1  1  1]\n",
      "  [ 1  1  1 ...  1  1  1]\n",
      "  ...\n",
      "  [ 1  1  1 ...  1  1  1]\n",
      "  [ 1  1  1 ...  1  1  1]\n",
      "  [ 1  1  1 ...  1  1  1]]\n",
      "\n",
      " [[ 1  1  1 ...  1  1  1]\n",
      "  [ 1  1  1 ...  1  1  1]\n",
      "  [ 1  1  1 ...  1  1  1]\n",
      "  ...\n",
      "  [ 1  1  1 ...  1  1  1]\n",
      "  [ 1  1  1 ...  1  1  1]\n",
      "  [ 1  1  1 ...  1  1  1]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1 -1 -1 ...  1 -1 -1]\n",
      "  [ 1 -1 -1 ...  1  1  1]\n",
      "  [ 1  1  1 ... -1 -1  1]\n",
      "  ...\n",
      "  [ 1 -1 -1 ...  1 -1 -1]\n",
      "  [ 1 -1 -1 ... -1 -1 -1]\n",
      "  [ 1 -1 -1 ... -1 -1 -1]]\n",
      "\n",
      " [[-1  1  1 ...  1 -1  1]\n",
      "  [-1  1  1 ...  1 -1  1]\n",
      "  [ 1  1  1 ...  1  1  1]\n",
      "  ...\n",
      "  [-1 -1 -1 ...  1  1 -1]\n",
      "  [ 1 -1 -1 ...  1  1 -1]\n",
      "  [ 1  1  1 ...  1 -1 -1]]\n",
      "\n",
      " [[-1 -1 -1 ...  1 -1  1]\n",
      "  [-1 -1 -1 ...  1  1  1]\n",
      "  [-1 -1  1 ...  1  1 -1]\n",
      "  ...\n",
      "  [-1  1 -1 ... -1 -1 -1]\n",
      "  [ 1 -1 -1 ...  1  1  1]\n",
      "  [-1 -1  1 ...  1  1 -1]]]\n"
     ]
    }
   ],
   "source": [
    "#Step 2: traverse the space of temp around the critical point and recover labels \n",
    "#to see if the system is largely coherent or decoherent wrt spins. Use this to build\n",
    "#a temperature boundary to the system\n",
    "\n",
    "x = np.linspace(1,4,100)\n",
    "isingLibD = []\n",
    "labelsD = []\n",
    "\n",
    "for i in x:\n",
    "    ising_simu = Ising(32, i)\n",
    "    isingLibD.append(ising_simu.simulate())\n",
    "    labelsD.append(i)\n",
    "    \n",
    "    \n",
    "libIsingA = np.array(isingLib[1000:-1])\n",
    "labelsIsingA = np.array(labels[1000:-1]) \n",
    "print(libIsingA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1024)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1.]\n",
      "Label:0.0 Temperature:2.3636363636363638\n",
      "Label:1.0 Temperature:2.393939393939394\n"
     ]
    }
   ],
   "source": [
    "libIsingA = np.array(isingLib[999:-1])\n",
    "labelsIsingA = np.array(labels[999:-1]) \n",
    "libIsingAtest \n",
    "print(libIsingA.reshape((-1, 32*32)).shape)\n",
    "modelLabels = classifierD.predict(libIsingA.reshape((-1, 32*32)))\n",
    "\n",
    "print(modelLabels)\n",
    "\n",
    "i = 46\n",
    "print(\"Label:{} Temperature:{}\".format(modelLabels[i], labelsIsingA[i]))\n",
    "\n",
    "i = 47\n",
    "print(\"Label:{} Temperature:{}\".format(modelLabels[i], labelsIsingA[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can see that the boundary for this is at about 2.38 degrees where the classifier crosses over. This is the prediction as to what the critical temperature is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======E=============E=============E=============E=============E=============E=============E=============E=============E========\n",
    "\n",
    "Finally, develop a method to estimate the temperature of the two pro\n",
    "vided datasets `PS2-d-highT' and `PS2-d-lowT', using whatever method\n",
    "you can think. [Hint: Can you get a parameter associated with the classifer that correlates with temperature? There are multiple possible ways\n",
    "of doing (d) and (e)] Using your method, report the two temperatures\n",
    "you estimate on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/?fbclid=IwAR3aemwOJ3jpoRH1mFUSMX9X2C5NLjVYTH_gi1jddEoYRgjcJGcEUj2IRy8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5452915346334124, 1.180695617854632, 4.318532557608271, 5.273172812612567, 4.35164771376987, 1.5177202337325262, 4.774645855089654, 1.7251350352458, 4.325554671446928, 5.088661772115013, 1.9130255985667843, 5.40982770566262, 3.8635370843060937, 5.722110952199214, 2.760879011656501, 6.792587143548234, 6.823093929820815, 6.60579020423822, 0.45941055542723386, 0.5161298423453066, 1.6725646226842006, 6.549029222888312, 0.13173304733387947, 3.110710819073188, 1.8652708951262058, 3.205180870152585, 5.133006785639505, 2.8584249244885305, 0.74001835292312, 5.317857536645009, 0.22794658764722886, 5.997947510034246, 1.8769881429586053, 0.318516018640534, 0.9980760133479101, 3.136038475267761, 3.160365273468174, 3.613598345552451, 3.1221855350752232, 1.1311658574596546, 3.1751231203372634, 4.151012473949594, 4.757784952372055, 5.935607330747797, 0.3665113713706438, 2.958845865104765, 0.8925308813562358, 6.419662137039225, 0.43904783346955223, 5.5421136855233915, 4.3194155140920865, 2.0538000105915466, 4.4754493052352124, 0.72213658032028, 6.398063497210417, 4.032091829745826, 0.185374751105566, 4.347195255353242, 0.7454407563009199, 3.2299940300756487, 1.7211331482369587, 2.890422946630906, 0.8959736944696038, 3.551809711474192, 3.141373677314513, 5.2922634794287795, 3.7380567089784837, 3.063076768613464, 0.5555301491472223, 6.850515567665533, 2.531285421281435, 3.095820302577617, 6.263967984466569, 3.0497394295434606, 4.732038493280152, 6.445668706629541, 3.099561133408573, 6.936860376623178, 4.901678120518123, 4.357131007374927, 6.590500331700117, 5.343878546612144, 4.650676619940734, 2.672247639344578, 2.857926897030211, 1.6947008296947943, 6.022527218697646, 1.8114709613410984, 0.7354220202073569, 3.5761657037817063, 5.057328303017871, 3.353604960448151, 2.2401851608996455, 4.002290864996253, 1.3794573015711058, 5.0446980687319956, 2.755433013186696, 5.543277790568484, 1.3521565858035496, 2.3822357045492266, 0.6009997259893488, 4.854953305631327, 0.46987667887965, 0.05650122192873719, 4.732485098869594, 6.509375923759881, 0.40720594085769257, 4.287748815327638, 2.017510830696029, 6.7843100941263215, 2.172038630257923, 6.340849782859504, 5.314883893894684, 3.2493563757587465, 2.022341428310725, 4.408946981001941, 6.999034010420987, 0.9101889310753996, 0.15890037313956829, 3.273422348755923, 6.007163550634911, 6.148862763003288, 4.283623022894544, 1.6329116045665275, 6.677167349865838, 6.655244546516281, 1.1203465636214571, 5.3977834066805075, 0.6147496648959542, 3.179096588835489, 2.2332500723565953, 1.065919508065627, 1.2661804116550308, 4.152844091371098, 4.539726691021583, 5.709340291967343, 3.31976906982149, 1.527595283785558, 6.758297221477956, 4.740269881632903, 0.17602907605038087, 0.39308121134708873, 4.2547825001962725, 1.8870675755042026, 6.221110474735707, 4.8362231376042, 4.763587312477462, 6.7233934795281876, 3.5583443509814305, 5.29806419583017, 2.9794424063639946, 1.6272900696293418, 0.8282128058943004, 4.980570742393864, 6.55324353522912, 0.08824367232564079, 4.239119406157631, 1.5667474101922183, 5.762732483135257, 4.469466144252667, 3.9584506936957324, 1.702741407020504, 0.909783936231435, 4.930703548973347, 1.8404933919912936, 6.732908622351015, 0.41986353086626826, 1.4721457085251612, 1.3784689929083433, 0.41305931696505016, 2.5911351418712147, 1.4693281531929903, 1.8890989835686678, 6.175968715003214, 5.864984081775538, 4.070438045067397, 5.4016012582313, 5.678675344057307, 2.735686933733941, 2.8412129959817847, 6.694768490556902, 1.2489655062491645, 3.5090027931693415, 2.766730772865527, 0.6803442205967893, 2.915729672649137, 2.064329757985722, 0.33491914235879894, 3.160097933135671, 6.247716160593572, 5.389378882824135, 2.072825615137987, 4.72850142591266, 6.476181558096442, 1.426263143870832, 1.0572898297256386, 4.817724110789416, 1.7332205331283976, 3.5287395419372594, 3.663642129922797, 2.786446578197462, 6.775288701416262, 4.598980129218099, 6.737663070129343, 4.051822381234903, 6.8174275133265105, 4.167433281972948, 1.6425476841825981, 3.7502612566491345, 5.069408740614502, 1.8445976016637222, 0.45543855005311784, 3.9688069966560526, 5.86209655281034, 1.5775143613625768, 0.9104066908135862, 6.4561913452256245, 6.1313296134000215, 5.766777801834399, 2.6732334827387425, 6.294747856377023, 2.9862393035800574, 0.9147537174889708, 2.819994642295922, 0.5030145035998198, 5.699929786984629, 0.9276654963442891, 1.2231473275094367, 5.56166859611847, 2.1825621081428377, 6.499033765095196, 4.666936657440272, 1.3083845319309555, 6.324625526334137, 0.7082399958760871, 5.176091145278278, 2.9982057649042937, 1.3444132562304039, 3.4092312083132468, 1.8437462041912607, 1.4033362431244436, 6.327446615329642, 3.5582369910419773, 0.7555798505025876, 5.981038876059027, 4.336763322973071, 2.6556709075843474, 4.432383554821705, 3.077605721968512, 6.57167247362605, 0.799615336187129, 6.99951937286231, 6.149018532181063, 5.419288590889884, 1.9376921672815115, 3.8042824456404305, 6.619198768030575, 5.4000807293440225, 5.884299778047054, 4.464586285156789, 4.171113977288694, 1.1677432058872774, 4.784390832246949, 1.3289606628329107, 4.51409517465457, 4.983567070787661, 1.2953550886312022, 2.246731983001736, 3.147425659383428, 4.315195474486468, 5.849751622933597, 2.9221784299594304, 3.0019247115680923, 6.1114740027785155, 2.1939977024716737, 4.994195558215969, 5.161898742852292, 3.3170064612717023, 1.702394921301178, 6.993289238287613, 4.145262301538642, 0.02544837296342428, 3.727211630411163, 4.198804120022471, 0.1862330746797075, 5.251899603995228, 3.0603752848089854, 5.169062681909021, 4.636740378568311, 3.4938409279918603, 3.911610771942467, 2.221082476783576, 1.82767682708264, 2.4766131738648904, 1.941165765957679, 1.0840223451525772, 0.507158180256633, 0.4538719391502354, 0.24020622851149698, 5.507382190458349, 0.8493303418897776, 0.9740972147296598, 3.455336639295863, 4.475273974906116, 6.578644646135187, 5.584454970593511, 0.5031869116430439, 1.2848900044352898, 0.9918774421447696, 5.7309971535329955, 0.48035968332302303, 3.7525929996040635, 2.1984254922625306, 1.5045241332927137, 0.16971244600077084, 3.5541079259618247, 1.6090055820058735, 6.008827579530167, 0.4407254480116227, 2.90113860958242, 5.023255302429946, 2.0214444693829594, 4.155266214581736, 2.8899027950865515, 1.0500460634389477, 0.2944958165530892, 2.1578515043998006, 3.3382801376124913, 4.919080172229162, 0.8552759098744297, 1.870112493444116, 3.624627314103752, 0.5544568416840708, 0.8101649372640645, 6.657798903869534, 4.248773231103791, 1.608171928672069, 0.11691477174919807, 4.1409117031256875, 4.587741340286372, 5.858751370744182, 5.9756813850587545, 1.6806465442131966, 4.301968647844461, 5.934775058782818, 5.373686139547325, 1.6765732644410374, 3.9578308968788556, 1.1853979572854452, 1.8887575647479737, 1.1639492795051234, 0.5831933743699043, 0.7330290855348714, 3.8278549257237935, 4.95609504039926, 4.022229329299684, 5.386055783506187, 1.0214574191435768, 0.4362435473139864, 6.358570899502448, 1.1071691225713813, 4.937110309145767, 0.19903758388542225, 2.7823602473965074, 4.2500092959573, 2.3188533607420125, 2.609278184807571, 5.416504936538199, 3.7605085178828865, 1.872392156218386, 3.3347241091385125, 0.9561269978746505, 6.101910451955657, 6.942179098388612, 6.893638612383248, 6.491699963854143, 6.419789080926509, 2.2050328515358455, 4.113379873871238, 2.807664483312191, 3.6473625662844293, 6.0999734590122054, 1.2402807682896615, 2.3926722436784966, 5.077748882907287, 4.738243950252714, 1.394480804658816, 1.055151584016252, 1.6736589279645264, 6.108466633456041, 5.197533072312955, 6.323699013314183, 0.34504936905250594, 4.2853557821114485, 2.064799404153349, 6.472536187515727, 4.491400874725753, 0.7075730650159363, 4.72202691225551, 0.06483365120002282, 2.2554548204757294, 6.518907816791277, 4.179312504267481, 3.523582466809427, 3.34875888184818, 5.3056107968559765, 1.5680355681792797, 2.0779356459089158, 4.596579618915451, 5.732297574772491, 0.619490326569208, 4.097418399366504, 3.592780240003206, 6.819930958750257, 4.094025811923289, 3.4577618418485687, 1.3967957850496688, 0.9155336666838365, 0.3884893660765768, 0.20684656080271913, 2.2689314203490842, 2.60230410774922, 6.41824637508865, 3.9179078464713566, 0.08702083631897994, 3.319456909468707, 6.47820518046208, 5.065618638079526, 3.3780903504235154, 4.615997105721415, 5.514886002117776, 2.7866411957355974, 0.9888009558237623, 5.84699418196714, 0.3305375800139282, 3.830486905654869, 0.7535727331286209, 2.004431736156775, 0.9419621097885429, 6.478926810822829, 4.444042759276042, 2.159949111356667, 4.198981921856165, 2.1065175520419936, 1.2615535868995496, 4.009638656364395, 0.36489919954871197, 5.739377613347419, 2.7880358791265754, 2.146343334661318, 1.0788411602422299, 4.863404469452399, 1.4443969238154917, 5.38496535728801, 2.416245612424688, 0.6172129174087728, 1.839772969077608, 4.87367607815439, 3.4831682934143906, 6.9684688284776355, 4.795883716743503, 0.5171318497767853, 4.3774158010612485, 1.2508779832970713, 1.311515892583306, 5.0022288146494365, 6.1752377517743255, 6.231684487201134, 5.460831898348583, 6.037224756933052, 6.211083879240121, 1.3092211684072717, 0.8960041475516681, 4.3967137262649585, 5.177008945422344, 3.1118037770288804, 5.225989145986144, 4.179220631368981, 2.1323314185760704, 2.1459710308203204, 5.485704421203604, 5.013024477566072, 2.173717329513803, 1.893116668605126, 0.45217954375152514, 5.452123501808072, 0.2339688224178038, 6.527348196009609, 5.668207974624535, 2.200875780426177, 6.192723459837622, 2.793205298189714, 5.203324020968009, 1.3462669832570628, 3.7588087700753245, 5.6051123036999035, 3.1219229111452433, 2.6256745582897545, 4.598627538227309, 1.1792877757983216, 1.6273224849741958, 3.679189108918492, 5.145146617178425, 2.3557993353969326, 1.0618499655995461, 0.20031776451344285, 0.9264214644303308, 4.991785441686379, 2.0857158701430905, 5.812000897750464, 4.838546328968711, 6.887192710305851, 3.8421976544097705, 3.1195053059006708, 6.162287843992314, 6.034484470614173, 0.2767139240279255, 6.8808677821761615, 5.639548816696639, 0.7240537618794328, 4.150496899362599, 4.220383950005774, 2.6474818931060833, 0.6716652824995248, 2.86372071947407, 1.1698878759789966, 1.3608089000770747, 2.817785921021497, 4.147711019810236, 5.9829756808789485, 3.0986644615253183, 3.4573587201913405, 5.449996117534864, 3.0064382293542913, 3.1944834830763194, 5.990895158810906, 3.8263143896307215, 2.962365482257013, 1.4955356829030564, 6.873377350470101, 0.6959362323459884, 4.857789981482093, 3.291634110696023, 5.727573895500756, 0.6488037730304734, 3.07920148188846, 1.9740504731304855, 0.9379428413288396, 1.8284142263743286, 1.311195681042134, 3.576207510571221, 1.4935481593005067, 0.6504680871285624, 1.70686594519564, 3.9917533165363706, 5.090198406281951, 3.9714206413568935, 4.849844547595668, 5.094744409547894, 4.2558610913581765, 4.128311750945835, 5.030817510819652, 2.3696355952359625, 3.554663878450859, 6.617264767472833, 1.2326864569782496, 3.420443622921637, 3.8628838247210187, 2.816014737272172, 3.45450871018952, 4.984094936053727, 3.080115958855812, 0.29608410942910834, 5.022689936995091, 5.210182483286533, 2.399797180494723, 1.3644354723499736, 0.47071990017249965, 0.0862726662641331, 0.9817716384538566, 5.463284145902212, 6.100451873377366, 5.71193484659917, 3.845058168171519, 0.36432459613444457, 6.930435892220869, 3.912007519650968, 0.4891167190837157, 1.40947095981353, 6.343713237995785, 5.844563595229629, 3.1473507788035873, 3.824897848432829, 2.7696535376463682, 4.32381042727501, 6.977004216477379, 3.557821694452823, 1.6380738289822405, 5.588623085058392, 4.38365869038905, 1.5466107932418554, 4.822778755142903, 6.25084541289664, 2.6003225290721885, 2.478751726558386, 5.760821287294439, 3.6491936877307594, 2.173121226139125, 1.0436921126653749, 2.6606383239304763, 6.831691318916728, 2.039853303059133, 6.561466543828978, 3.495108511002478, 6.044717887443805, 2.8597144641017005, 2.4319365296806597, 5.070331104081536, 4.141458390493207, 5.21607404681195, 0.28423685844164637, 2.8388637020851335, 5.42835888700916, 4.752414015777095, 6.01736390697139, 4.588553142757486, 5.075143307196504, 6.294548363649798, 2.8046716982384403, 5.025014509733797, 6.705113595531317, 2.3633296461981352, 3.685684102382515, 1.242546775985713, 3.728142478692753, 5.882412940837421, 1.8494077210613837, 5.813364914712735, 0.5937847011359324, 0.5481372703549362, 3.7733380670017627, 6.6079951557250425, 4.733454716630107, 6.452558191271622, 4.5735089164846086, 0.15020387158334503, 0.8456549035660795, 6.803675484531764, 6.529648939822143, 0.08919175415197078, 0.18505389746751, 2.7948259112678104, 3.7396810277931434, 1.7220013007111228, 6.216555976299007, 6.760144089777586, 1.4909713005482432, 2.9324583985833983, 0.9441132300644901, 3.8439013310880794, 0.4087828879140937, 1.1837887004710057, 0.18061951701277912, 4.525094603546266, 6.761706067582269, 2.2453398582391983, 2.7517211963734054, 5.250281422723411, 2.082992848475758, 5.751667875040011, 6.684072745093394, 6.983331912765682, 0.2758339475984861, 2.7724745757350697, 0.25602900307678633, 4.1808018389877635, 2.634616708121941, 6.869253674059806, 1.330939365886567, 3.7595793628615493, 3.5480644552116987, 2.1847326859540552, 3.0965370227575617, 4.072713528112199, 5.522989670461404, 5.1200116593319915, 1.6880075274806892, 0.3022812824789217, 6.5605908251759555, 5.572746719443613, 0.47857985680111914, 1.6089063350930313, 4.70155968092707, 2.031358878390595, 0.9226773962985281, 0.4003325174550041, 1.2900724795626375, 1.2231151460128031, 4.920387119896988, 0.9480274166190357, 5.807691090798244, 4.948189325912487, 3.3310812452212355, 2.7703827530300966, 2.99385273341741, 5.241389544100475, 3.323072958860874, 2.7783027964377536, 4.325715277017839, 6.069265247729883, 6.317330811751839, 2.68499199077265, 6.47466069747346, 0.17268716559989372, 1.7861930594535333, 0.38056030596851886, 3.045950467355397, 4.896186772847161, 4.99726595312023, 4.229114708865506, 2.5997826593977456, 4.086337317814227, 2.0719406194970142, 5.054263821781401, 2.510829089998524, 1.5680252062810052, 3.925257121582826, 1.4267577861928409, 1.6759048822188372, 6.433792831530738, 1.9412712835678234, 2.6716070341479083, 0.24472641334489786, 5.583903649014429, 6.497306176938549, 0.9313297754106629, 1.0119921724172165, 0.7714884684583867, 0.8654776193507648, 6.143387444723103, 6.488910615430248, 6.140807557057945, 2.1368317864189224, 2.332763729426506, 4.93794069386116, 5.706901040505174, 6.152320894459772, 5.499055031162749, 3.1529091006780225, 2.365608733464364, 3.515788233258256, 3.280477403351287, 1.9722706972420374, 1.3391880022299878, 3.3120389113847715, 6.740020729005373, 2.6533901402508246, 5.398422909266225, 0.7176321850467561, 4.424027353640125, 3.798404882890958, 1.894969160196073, 2.515097919641493, 1.4914050545099347, 0.8382770655691403, 4.721880516670212, 3.5570156758806104, 1.2560642269694546, 1.2725011822929826, 2.220432337292916, 4.166768382632432, 3.895927008328398, 6.72997535478545, 6.476798824825088, 1.5340941274966668, 1.4019418564407746, 3.25134826305786, 3.612692264135915, 6.121100191723325, 1.368759253091799, 1.2159944189401828, 3.066298589491202, 3.569864925144864, 2.9934725983485553, 4.570468335512875, 4.0346474179506036, 6.061168349106455, 3.66084729279765, 4.271783670005904, 5.327243002715986, 6.9786857157658115, 0.6285451359461012, 2.9813970875702185, 5.518979512854729, 5.229817042135579, 6.939893771366593, 1.033835414220353, 3.9730375086604295, 3.548916948678955, 6.160010663881665, 5.935766621614573, 5.342377618970652, 4.091240238855535, 4.455602550365626, 2.369065807956924, 1.1835007044628658, 1.524927650358713, 2.032942501321838, 6.118158869787131, 5.607452025360708, 6.90228573184578, 1.0531231820242866, 5.803804364849794, 4.147864381551427, 4.236652111559232, 0.2056635852478106, 0.5683955398999796, 4.225063830044357, 1.7513582613651626, 6.290267827060753, 0.32496837557695424, 4.450902820742968, 5.794811248037692, 4.222408955560169, 4.675923309905418, 4.567372662662622, 5.673391954084038, 2.04041746563201, 3.848737010101154, 1.991004762602047, 3.63644610164047, 1.8866259154118725, 3.9451787458600824, 6.157369766250932, 5.173158988525812, 4.3948504240365285, 1.6970991798243518, 5.022366010346864, 6.477973480971668, 0.6843170645412633, 2.330564927486641, 3.584730562773786, 1.1091165346080398, 1.2287146787061434, 3.1372318843471847, 1.038138630289825, 1.5602065135862768, 2.8991200629484144, 0.9035007692293443, 6.2812257833875, 4.236392675041978, 1.158361389249864, 4.687943400387771, 3.177559373481833, 4.829512188054312, 5.087965931603959, 6.580836170636272, 3.994881899324067, 3.0914363296083325, 6.3491975048797045, 2.5781207558294814, 1.0997614521330006, 6.480802465523978, 6.823270359839747, 6.840189698540902, 1.9214828873684118, 6.879090936383431, 6.074479871247646, 5.84251050092651, 1.0823736166871452, 6.099447484826393, 4.63517864151821, 0.7642740215030697, 1.5127597508023993, 6.736205275324632, 2.4257510373477578, 0.6486242713946543, 2.7153284432127105, 4.964038790970595, 1.5449241738651247, 0.4048213535113656, 0.42048604756983265, 0.19680008987204112, 4.407566058227198, 0.888892534950571, 6.143231546815116, 5.814659863738652, 5.832408920964804, 3.720643927233825, 4.124441180538947, 3.816354481616501, 1.962702040327189, 5.604892661223612, 3.1997967361368422, 0.6736606057943082, 2.392160622266287, 6.188138457090923, 4.477605041657062, 5.981009100167361, 1.2290423418858256, 2.355339339260598, 0.7323830722788881, 3.4670286545976934, 4.540053938223214, 1.7924435608558025, 1.9062706412350723, 2.468312921213041, 5.571087398600196, 5.138297211377207, 2.7601853144326145, 2.3968711376121257, 6.040150034664661, 6.693840281651866, 1.4645168665359143, 2.3355026358272637, 0.08647946734900958, 5.427454521765806, 3.5279477370226218, 0.05079845786457393, 2.0118316092876696, 4.185145314954024, 3.690536539793064, 0.43433809001607937, 2.305656302523887, 2.9826092018490042, 2.1307132635428236, 6.696185163792901, 6.951040733297484, 3.5549701102808084, 0.9058858858242702, 6.928656264671261, 3.0551882623412947, 5.402790694336527, 1.9766639666172325, 3.1904820454778275, 6.4063309069112755, 2.8372381973708927, 2.970274662085757, 2.898534190545549, 4.825352710071842, 0.7837284624339417, 4.974900932366873, 3.3197430075364016, 3.1651576309376988, 3.8568628262864237, 6.910809429375399, 1.4997006216672206, 1.9486779303995, 4.023189741847659, 1.0818351854846702, 6.964174668308512, 5.015823882894906, 4.107952751385851, 1.4782941576091166, 0.23178107882146393, 3.9296792025456657, 4.338874111265082, 5.448456576615731, 0.6093107960385246, 3.1728602188354413, 6.362284519015802, 5.733625958975681, 4.131896125947228, 3.5856795048024748, 2.167081147586147, 5.609483820266373, 5.471892701958439, 1.575357485263379, 1.103971296744627, 3.9332369880112603, 5.4007738550288185, 0.07900264166394233, 6.933825879031976, 6.908318782707513, 6.792707931040356, 0.9240357004985992, 6.856000358841195, 6.69595811494828, 0.8417034621615231, 4.192781262171056, 6.143480704436485, 5.696442374722971, 0.45858653443444264, 3.9560115603700425, 5.090959616003131, 5.57013010202066, 5.046095240082131, 2.0519637651215983, 3.8815329043578592, 5.786619535298163, 5.071206626965131, 0.14757510959144993, 3.4469468207154996, 1.4308170634269302, 1.8081415387820508, 0.49602677466182865, 4.732817468263193, 5.2094223461866465, 0.2054099731189999, 4.306561330305313]\n"
     ]
    }
   ],
   "source": [
    "#generate the Ising library with corresponding temperature labels\n",
    "\n",
    "#DONT RUN THIS AGAIN \n",
    "isingLib = []\n",
    "labels = []\n",
    "for i in range(1000):\n",
    "    tTemp = np.random.rand()*7\n",
    "    ising_simu = Ising(32, tTemp)\n",
    "    isingLib.append(ising_simu.simulate())\n",
    "    labels.append(tTemp)\n",
    "    \n",
    "    \n",
    "libIsing = np.array(isingLib)\n",
    "labelsIsing = np.array(labels) \n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02544837296342428\n"
     ]
    }
   ],
   "source": [
    "print(np.ndarray.min(labelsIsing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build neural net to take the flattened images and project to temperature space\n",
    "\n",
    "# define base model\n",
    "def baseline_model():    \n",
    "    #source: https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/?fbclid=IwAR3aemwOJ3jpoRH1mFUSMX9X2C5NLjVYTH_gi1jddEoYRgjcJGcEUj2IRy8\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(32, 32)))\n",
    "    model.add(tf.keras.layers.Dense(1024, input_dim=1, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=200, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32)\n",
      "(1000,)\n",
      "Epoch 1/100\n",
      "900/900 [==============================] - 9s 10ms/step - loss: 6.3338\n",
      "Epoch 2/100\n",
      "900/900 [==============================] - 1s 601us/step - loss: 1.2181\n",
      "Epoch 3/100\n",
      "900/900 [==============================] - 0s 497us/step - loss: 0.5648\n",
      "Epoch 4/100\n",
      "900/900 [==============================] - 0s 445us/step - loss: 0.2502\n",
      "Epoch 5/100\n",
      "900/900 [==============================] - 0s 386us/step - loss: 0.1251\n",
      "Epoch 6/100\n",
      "900/900 [==============================] - 0s 401us/step - loss: 0.0878\n",
      "Epoch 7/100\n",
      "900/900 [==============================] - 0s 371us/step - loss: 0.0587\n",
      "Epoch 8/100\n",
      "900/900 [==============================] - 0s 341us/step - loss: 0.0460\n",
      "Epoch 9/100\n",
      "900/900 [==============================] - 0s 346us/step - loss: 0.0410\n",
      "Epoch 10/100\n",
      "900/900 [==============================] - 0s 324us/step - loss: 0.0310\n",
      "Epoch 11/100\n",
      "900/900 [==============================] - 0s 328us/step - loss: 0.0244\n",
      "Epoch 12/100\n",
      "900/900 [==============================] - 0s 301us/step - loss: 0.0284\n",
      "Epoch 13/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0254\n",
      "Epoch 14/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0215\n",
      "Epoch 15/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0186\n",
      "Epoch 16/100\n",
      "900/900 [==============================] - 0s 276us/step - loss: 0.0195\n",
      "Epoch 17/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0219\n",
      "Epoch 18/100\n",
      "900/900 [==============================] - 0s 282us/step - loss: 0.0191\n",
      "Epoch 19/100\n",
      "900/900 [==============================] - 0s 281us/step - loss: 0.0219\n",
      "Epoch 20/100\n",
      "900/900 [==============================] - 0s 289us/step - loss: 0.0238\n",
      "Epoch 21/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0243\n",
      "Epoch 22/100\n",
      "900/900 [==============================] - 0s 332us/step - loss: 0.0280\n",
      "Epoch 23/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0144\n",
      "Epoch 24/100\n",
      "900/900 [==============================] - 0s 318us/step - loss: 0.0177\n",
      "Epoch 25/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0151\n",
      "Epoch 26/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0133\n",
      "Epoch 27/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0136\n",
      "Epoch 28/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0115\n",
      "Epoch 29/100\n",
      "900/900 [==============================] - 0s 292us/step - loss: 0.0106\n",
      "Epoch 30/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0140\n",
      "Epoch 31/100\n",
      "900/900 [==============================] - 0s 287us/step - loss: 0.0173\n",
      "Epoch 32/100\n",
      "900/900 [==============================] - 0s 278us/step - loss: 0.0183\n",
      "Epoch 33/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0150\n",
      "Epoch 34/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0150\n",
      "Epoch 35/100\n",
      "900/900 [==============================] - 0s 321us/step - loss: 0.0129\n",
      "Epoch 36/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0127\n",
      "Epoch 37/100\n",
      "900/900 [==============================] - 0s 279us/step - loss: 0.0126\n",
      "Epoch 38/100\n",
      "900/900 [==============================] - 0s 277us/step - loss: 0.0158\n",
      "Epoch 39/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0194\n",
      "Epoch 40/100\n",
      "900/900 [==============================] - 0s 318us/step - loss: 0.0181\n",
      "Epoch 41/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0289\n",
      "Epoch 42/100\n",
      "900/900 [==============================] - 0s 315us/step - loss: 0.0330\n",
      "Epoch 43/100\n",
      "900/900 [==============================] - 0s 324us/step - loss: 0.0384\n",
      "Epoch 44/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0323\n",
      "Epoch 45/100\n",
      "900/900 [==============================] - 0s 288us/step - loss: 0.0265\n",
      "Epoch 46/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0290\n",
      "Epoch 47/100\n",
      "900/900 [==============================] - 0s 319us/step - loss: 0.0340\n",
      "Epoch 48/100\n",
      "900/900 [==============================] - 0s 289us/step - loss: 0.0306\n",
      "Epoch 49/100\n",
      "900/900 [==============================] - 0s 289us/step - loss: 0.0266\n",
      "Epoch 50/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0288\n",
      "Epoch 51/100\n",
      "900/900 [==============================] - 0s 292us/step - loss: 0.0322\n",
      "Epoch 52/100\n",
      "900/900 [==============================] - 0s 329us/step - loss: 0.0353\n",
      "Epoch 53/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0446\n",
      "Epoch 54/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0382\n",
      "Epoch 55/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0431\n",
      "Epoch 56/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0310\n",
      "Epoch 57/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0283\n",
      "Epoch 58/100\n",
      "900/900 [==============================] - 0s 288us/step - loss: 0.0219\n",
      "Epoch 59/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0184\n",
      "Epoch 60/100\n",
      "900/900 [==============================] - 0s 278us/step - loss: 0.0185\n",
      "Epoch 61/100\n",
      "900/900 [==============================] - 0s 315us/step - loss: 0.0201\n",
      "Epoch 62/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0250\n",
      "Epoch 63/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0178\n",
      "Epoch 64/100\n",
      "900/900 [==============================] - 0s 332us/step - loss: 0.0186\n",
      "Epoch 65/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0157\n",
      "Epoch 66/100\n",
      "900/900 [==============================] - 0s 342us/step - loss: 0.0211\n",
      "Epoch 67/100\n",
      "900/900 [==============================] - 0s 287us/step - loss: 0.0205\n",
      "Epoch 68/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0229\n",
      "Epoch 69/100\n",
      "900/900 [==============================] - 0s 297us/step - loss: 0.0323\n",
      "Epoch 70/100\n",
      "900/900 [==============================] - 0s 317us/step - loss: 0.0304\n",
      "Epoch 71/100\n",
      "900/900 [==============================] - 0s 281us/step - loss: 0.0369\n",
      "Epoch 72/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0252\n",
      "Epoch 73/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0225\n",
      "Epoch 74/100\n",
      "900/900 [==============================] - 0s 289us/step - loss: 0.0216\n",
      "Epoch 75/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0249\n",
      "Epoch 76/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0287\n",
      "Epoch 77/100\n",
      "900/900 [==============================] - 0s 310us/step - loss: 0.0336\n",
      "Epoch 78/100\n",
      "900/900 [==============================] - 0s 300us/step - loss: 0.0381\n",
      "Epoch 79/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0319\n",
      "Epoch 80/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0276\n",
      "Epoch 81/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0282\n",
      "Epoch 82/100\n",
      "900/900 [==============================] - 0s 297us/step - loss: 0.0365\n",
      "Epoch 83/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0440\n",
      "Epoch 84/100\n",
      "900/900 [==============================] - 0s 333us/step - loss: 0.0368\n",
      "Epoch 85/100\n",
      "900/900 [==============================] - 0s 312us/step - loss: 0.0428\n",
      "Epoch 86/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0352\n",
      "Epoch 87/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0396\n",
      "Epoch 88/100\n",
      "900/900 [==============================] - 0s 320us/step - loss: 0.0375\n",
      "Epoch 89/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0417\n",
      "Epoch 90/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0384\n",
      "Epoch 91/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0469\n",
      "Epoch 92/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0461\n",
      "Epoch 93/100\n",
      "900/900 [==============================] - 0s 301us/step - loss: 0.0511\n",
      "Epoch 94/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0424\n",
      "Epoch 95/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0385\n",
      "Epoch 96/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0404\n",
      "Epoch 97/100\n",
      "900/900 [==============================] - 0s 310us/step - loss: 0.0371\n",
      "Epoch 98/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0345\n",
      "Epoch 99/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0312\n",
      "Epoch 100/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0237\n",
      "100/100 [==============================] - 2s 23ms/step\n",
      "Epoch 1/100\n",
      "900/900 [==============================] - 9s 10ms/step - loss: 6.7298\n",
      "Epoch 2/100\n",
      "900/900 [==============================] - 1s 631us/step - loss: 1.3115\n",
      "Epoch 3/100\n",
      "900/900 [==============================] - 0s 518us/step - loss: 0.5937\n",
      "Epoch 4/100\n",
      "900/900 [==============================] - 0s 457us/step - loss: 0.2223\n",
      "Epoch 5/100\n",
      "900/900 [==============================] - 0s 407us/step - loss: 0.1104\n",
      "Epoch 6/100\n",
      "900/900 [==============================] - 0s 382us/step - loss: 0.0710\n",
      "Epoch 7/100\n",
      "900/900 [==============================] - 0s 374us/step - loss: 0.0497\n",
      "Epoch 8/100\n",
      "900/900 [==============================] - 0s 349us/step - loss: 0.0358\n",
      "Epoch 9/100\n",
      "900/900 [==============================] - 0s 347us/step - loss: 0.0299\n",
      "Epoch 10/100\n",
      "900/900 [==============================] - 0s 320us/step - loss: 0.0295\n",
      "Epoch 11/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0231\n",
      "Epoch 12/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0218\n",
      "Epoch 13/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0186\n",
      "Epoch 14/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0166\n",
      "Epoch 15/100\n",
      "900/900 [==============================] - 0s 281us/step - loss: 0.0139\n",
      "Epoch 16/100\n",
      "900/900 [==============================] - 0s 279us/step - loss: 0.0147\n",
      "Epoch 17/100\n",
      "900/900 [==============================] - 0s 280us/step - loss: 0.0142\n",
      "Epoch 18/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0128\n",
      "Epoch 19/100\n",
      "900/900 [==============================] - 0s 279us/step - loss: 0.0141\n",
      "Epoch 20/100\n",
      "900/900 [==============================] - 0s 278us/step - loss: 0.0146\n",
      "Epoch 21/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0146\n",
      "Epoch 22/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0137\n",
      "Epoch 23/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0126\n",
      "Epoch 24/100\n",
      "900/900 [==============================] - 0s 287us/step - loss: 0.0163\n",
      "Epoch 25/100\n",
      "900/900 [==============================] - 0s 285us/step - loss: 0.0189\n",
      "Epoch 26/100\n",
      "900/900 [==============================] - 0s 288us/step - loss: 0.0214\n",
      "Epoch 27/100\n",
      "900/900 [==============================] - 0s 276us/step - loss: 0.0335\n",
      "Epoch 28/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0202\n",
      "Epoch 29/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0181\n",
      "Epoch 30/100\n",
      "900/900 [==============================] - 0s 292us/step - loss: 0.0240\n",
      "Epoch 31/100\n",
      "900/900 [==============================] - 0s 274us/step - loss: 0.0189\n",
      "Epoch 32/100\n",
      "900/900 [==============================] - 0s 281us/step - loss: 0.0169\n",
      "Epoch 33/100\n",
      "900/900 [==============================] - 0s 281us/step - loss: 0.0195\n",
      "Epoch 34/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0156\n",
      "Epoch 35/100\n",
      "900/900 [==============================] - 0s 289us/step - loss: 0.0182\n",
      "Epoch 36/100\n",
      "900/900 [==============================] - 0s 288us/step - loss: 0.0157\n",
      "Epoch 37/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0148\n",
      "Epoch 38/100\n",
      "900/900 [==============================] - 0s 278us/step - loss: 0.0174\n",
      "Epoch 39/100\n",
      "900/900 [==============================] - 0s 287us/step - loss: 0.0214\n",
      "Epoch 40/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0175\n",
      "Epoch 41/100\n",
      "900/900 [==============================] - 0s 313us/step - loss: 0.0193\n",
      "Epoch 42/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0171\n",
      "Epoch 43/100\n",
      "900/900 [==============================] - 0s 281us/step - loss: 0.0165\n",
      "Epoch 44/100\n",
      "900/900 [==============================] - 0s 288us/step - loss: 0.0200\n",
      "Epoch 45/100\n",
      "900/900 [==============================] - 0s 287us/step - loss: 0.0182\n",
      "Epoch 46/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0236\n",
      "Epoch 47/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0189\n",
      "Epoch 48/100\n",
      "900/900 [==============================] - 0s 292us/step - loss: 0.0177\n",
      "Epoch 49/100\n",
      "900/900 [==============================] - 0s 301us/step - loss: 0.0167\n",
      "Epoch 50/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0168\n",
      "Epoch 51/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0166\n",
      "Epoch 52/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0214\n",
      "Epoch 53/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0247\n",
      "Epoch 54/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0212\n",
      "Epoch 55/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0222\n",
      "Epoch 56/100\n",
      "900/900 [==============================] - 0s 280us/step - loss: 0.0189\n",
      "Epoch 57/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0198\n",
      "Epoch 58/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0292\n",
      "Epoch 59/100\n",
      "900/900 [==============================] - 0s 285us/step - loss: 0.0324\n",
      "Epoch 60/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0223\n",
      "Epoch 61/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0228\n",
      "Epoch 62/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0249\n",
      "Epoch 63/100\n",
      "900/900 [==============================] - 0s 297us/step - loss: 0.0234\n",
      "Epoch 64/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0234\n",
      "Epoch 65/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0272\n",
      "Epoch 66/100\n",
      "900/900 [==============================] - 0s 278us/step - loss: 0.0193\n",
      "Epoch 67/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0172\n",
      "Epoch 68/100\n",
      "900/900 [==============================] - 0s 300us/step - loss: 0.0161\n",
      "Epoch 69/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0166\n",
      "Epoch 70/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0140\n",
      "Epoch 71/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0228\n",
      "Epoch 72/100\n",
      "900/900 [==============================] - 0s 288us/step - loss: 0.0201\n",
      "Epoch 73/100\n",
      "900/900 [==============================] - 0s 288us/step - loss: 0.0221\n",
      "Epoch 74/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0236\n",
      "Epoch 75/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0301\n",
      "Epoch 76/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0399\n",
      "Epoch 77/100\n",
      "900/900 [==============================] - 0s 312us/step - loss: 0.0371\n",
      "Epoch 78/100\n",
      "900/900 [==============================] - 0s 329us/step - loss: 0.0398\n",
      "Epoch 79/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0377\n",
      "Epoch 80/100\n",
      "900/900 [==============================] - 0s 321us/step - loss: 0.0378\n",
      "Epoch 81/100\n",
      "900/900 [==============================] - 0s 315us/step - loss: 0.0325\n",
      "Epoch 82/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0328\n",
      "Epoch 83/100\n",
      "900/900 [==============================] - 0s 302us/step - loss: 0.0284\n",
      "Epoch 84/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0295\n",
      "Epoch 85/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0278\n",
      "Epoch 86/100\n",
      "900/900 [==============================] - 0s 297us/step - loss: 0.0240\n",
      "Epoch 87/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0194\n",
      "Epoch 88/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0176\n",
      "Epoch 89/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0197\n",
      "Epoch 90/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0227\n",
      "Epoch 91/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0260\n",
      "Epoch 92/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0227\n",
      "Epoch 94/100\n",
      "900/900 [==============================] - 0s 278us/step - loss: 0.0193\n",
      "Epoch 95/100\n",
      "900/900 [==============================] - 0s 276us/step - loss: 0.0167\n",
      "Epoch 96/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0187\n",
      "Epoch 97/100\n",
      "900/900 [==============================] - 0s 321us/step - loss: 0.0194\n",
      "Epoch 98/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0162\n",
      "Epoch 99/100\n",
      "900/900 [==============================] - 0s 273us/step - loss: 0.0143\n",
      "Epoch 100/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0156\n",
      "100/100 [==============================] - 3s 25ms/step\n",
      "Epoch 1/100\n",
      "900/900 [==============================] - 9s 10ms/step - loss: 6.3049\n",
      "Epoch 2/100\n",
      "900/900 [==============================] - 0s 424us/step - loss: 1.5700\n",
      "Epoch 3/100\n",
      "900/900 [==============================] - 0s 410us/step - loss: 0.6175\n",
      "Epoch 4/100\n",
      "900/900 [==============================] - 0s 410us/step - loss: 0.3048\n",
      "Epoch 5/100\n",
      "900/900 [==============================] - 0s 392us/step - loss: 0.1642\n",
      "Epoch 6/100\n",
      "900/900 [==============================] - 0s 335us/step - loss: 0.1009\n",
      "Epoch 7/100\n",
      "900/900 [==============================] - 0s 367us/step - loss: 0.0618\n",
      "Epoch 8/100\n",
      "900/900 [==============================] - 0s 325us/step - loss: 0.0550\n",
      "Epoch 9/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0425\n",
      "Epoch 10/100\n",
      "900/900 [==============================] - 0s 343us/step - loss: 0.0373\n",
      "Epoch 11/100\n",
      "900/900 [==============================] - 0s 327us/step - loss: 0.0243\n",
      "Epoch 12/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0222\n",
      "Epoch 13/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0182\n",
      "Epoch 14/100\n",
      "900/900 [==============================] - 0s 339us/step - loss: 0.0171\n",
      "Epoch 15/100\n",
      "900/900 [==============================] - 0s 321us/step - loss: 0.0176\n",
      "Epoch 16/100\n",
      "900/900 [==============================] - 0s 354us/step - loss: 0.0210\n",
      "Epoch 17/100\n",
      "900/900 [==============================] - 0s 352us/step - loss: 0.0193\n",
      "Epoch 18/100\n",
      "900/900 [==============================] - 0s 349us/step - loss: 0.0199\n",
      "Epoch 19/100\n",
      "900/900 [==============================] - 0s 327us/step - loss: 0.0366\n",
      "Epoch 20/100\n",
      "900/900 [==============================] - 0s 332us/step - loss: 0.0316\n",
      "Epoch 21/100\n",
      "900/900 [==============================] - 0s 301us/step - loss: 0.0197\n",
      "Epoch 22/100\n",
      "900/900 [==============================] - 0s 325us/step - loss: 0.0175\n",
      "Epoch 23/100\n",
      "900/900 [==============================] - 0s 315us/step - loss: 0.0181\n",
      "Epoch 24/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0119\n",
      "Epoch 25/100\n",
      "900/900 [==============================] - 0s 347us/step - loss: 0.0127\n",
      "Epoch 26/100\n",
      "900/900 [==============================] - 0s 336us/step - loss: 0.0118\n",
      "Epoch 27/100\n",
      "900/900 [==============================] - 0s 320us/step - loss: 0.0123\n",
      "Epoch 28/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0145\n",
      "Epoch 29/100\n",
      "900/900 [==============================] - 0s 321us/step - loss: 0.0146\n",
      "Epoch 30/100\n",
      "900/900 [==============================] - 0s 324us/step - loss: 0.0125\n",
      "Epoch 31/100\n",
      "900/900 [==============================] - 0s 348us/step - loss: 0.0124\n",
      "Epoch 32/100\n",
      "900/900 [==============================] - 0s 319us/step - loss: 0.0118\n",
      "Epoch 33/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0157\n",
      "Epoch 34/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0167\n",
      "Epoch 35/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0174\n",
      "Epoch 36/100\n",
      "900/900 [==============================] - 0s 322us/step - loss: 0.0141\n",
      "Epoch 37/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0134\n",
      "Epoch 38/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0174\n",
      "Epoch 39/100\n",
      "900/900 [==============================] - 0s 325us/step - loss: 0.0120\n",
      "Epoch 40/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0149\n",
      "Epoch 41/100\n",
      "900/900 [==============================] - 0s 318us/step - loss: 0.0124\n",
      "Epoch 42/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0113\n",
      "Epoch 43/100\n",
      "900/900 [==============================] - 0s 312us/step - loss: 0.0105\n",
      "Epoch 44/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0150\n",
      "Epoch 45/100\n",
      "900/900 [==============================] - 0s 301us/step - loss: 0.0170\n",
      "Epoch 46/100\n",
      "900/900 [==============================] - 0s 280us/step - loss: 0.0162\n",
      "Epoch 47/100\n",
      "900/900 [==============================] - 0s 285us/step - loss: 0.0125\n",
      "Epoch 48/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0107\n",
      "Epoch 49/100\n",
      "900/900 [==============================] - 0s 300us/step - loss: 0.0107\n",
      "Epoch 50/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0113\n",
      "Epoch 51/100\n",
      "900/900 [==============================] - 0s 285us/step - loss: 0.0139\n",
      "Epoch 52/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0188\n",
      "Epoch 53/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0159\n",
      "Epoch 54/100\n",
      "900/900 [==============================] - 0s 277us/step - loss: 0.0194\n",
      "Epoch 55/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0153\n",
      "Epoch 56/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0220\n",
      "Epoch 57/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0203\n",
      "Epoch 58/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0162\n",
      "Epoch 59/100\n",
      "900/900 [==============================] - 0s 289us/step - loss: 0.0194\n",
      "Epoch 60/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0254\n",
      "Epoch 61/100\n",
      "900/900 [==============================] - 0s 297us/step - loss: 0.0209\n",
      "Epoch 62/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0206\n",
      "Epoch 63/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0319\n",
      "Epoch 64/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0482\n",
      "Epoch 65/100\n",
      "900/900 [==============================] - 0s 285us/step - loss: 0.0397\n",
      "Epoch 66/100\n",
      "900/900 [==============================] - 0s 287us/step - loss: 0.0515\n",
      "Epoch 67/100\n",
      "900/900 [==============================] - 0s 285us/step - loss: 0.0529\n",
      "Epoch 68/100\n",
      "900/900 [==============================] - 0s 312us/step - loss: 0.0419\n",
      "Epoch 69/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0414\n",
      "Epoch 70/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0298\n",
      "Epoch 71/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0338\n",
      "Epoch 72/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0425\n",
      "Epoch 73/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0385\n",
      "Epoch 74/100\n",
      "900/900 [==============================] - 0s 292us/step - loss: 0.0364\n",
      "Epoch 75/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0330\n",
      "Epoch 76/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0290\n",
      "Epoch 77/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0294\n",
      "Epoch 78/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0234\n",
      "Epoch 79/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0223\n",
      "Epoch 80/100\n",
      "900/900 [==============================] - 0s 313us/step - loss: 0.0164\n",
      "Epoch 81/100\n",
      "900/900 [==============================] - 0s 322us/step - loss: 0.0160\n",
      "Epoch 82/100\n",
      "900/900 [==============================] - 0s 292us/step - loss: 0.0161\n",
      "Epoch 83/100\n",
      "900/900 [==============================] - 0s 317us/step - loss: 0.0173\n",
      "Epoch 84/100\n",
      "900/900 [==============================] - 0s 321us/step - loss: 0.0146\n",
      "Epoch 85/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0152\n",
      "Epoch 86/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0140\n",
      "Epoch 87/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0201\n",
      "Epoch 88/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0180\n",
      "Epoch 89/100\n",
      "900/900 [==============================] - 0s 281us/step - loss: 0.0159\n",
      "Epoch 90/100\n",
      "900/900 [==============================] - 0s 282us/step - loss: 0.0125\n",
      "Epoch 91/100\n",
      "900/900 [==============================] - 0s 276us/step - loss: 0.0155\n",
      "Epoch 92/100\n",
      "900/900 [==============================] - 0s 277us/step - loss: 0.0286\n",
      "Epoch 93/100\n",
      "900/900 [==============================] - 0s 279us/step - loss: 0.0351\n",
      "Epoch 94/100\n",
      "900/900 [==============================] - 0s 285us/step - loss: 0.0252\n",
      "Epoch 95/100\n",
      "900/900 [==============================] - 0s 280us/step - loss: 0.0226\n",
      "Epoch 96/100\n",
      "900/900 [==============================] - 0s 285us/step - loss: 0.0190\n",
      "Epoch 97/100\n",
      "900/900 [==============================] - 0s 328us/step - loss: 0.0205\n",
      "Epoch 98/100\n",
      "900/900 [==============================] - 0s 320us/step - loss: 0.0180\n",
      "Epoch 99/100\n",
      "900/900 [==============================] - 0s 342us/step - loss: 0.0153\n",
      "Epoch 100/100\n",
      "900/900 [==============================] - 0s 319us/step - loss: 0.0184\n",
      "100/100 [==============================] - 1s 14ms/step\n",
      "Epoch 1/100\n",
      "900/900 [==============================] - 10s 11ms/step - loss: 4.1908\n",
      "Epoch 2/100\n",
      "900/900 [==============================] - 1s 633us/step - loss: 0.9637\n",
      "Epoch 3/100\n",
      "900/900 [==============================] - 1s 632us/step - loss: 0.3761\n",
      "Epoch 4/100\n",
      "900/900 [==============================] - 1s 574us/step - loss: 0.1578\n",
      "Epoch 5/100\n",
      "900/900 [==============================] - 0s 440us/step - loss: 0.0912\n",
      "Epoch 6/100\n",
      "900/900 [==============================] - 0s 507us/step - loss: 0.0579\n",
      "Epoch 7/100\n",
      "900/900 [==============================] - 0s 440us/step - loss: 0.0411\n",
      "Epoch 8/100\n",
      "900/900 [==============================] - 0s 347us/step - loss: 0.0359\n",
      "Epoch 9/100\n",
      "900/900 [==============================] - 0s 315us/step - loss: 0.0345\n",
      "Epoch 10/100\n",
      "900/900 [==============================] - 0s 393us/step - loss: 0.0287\n",
      "Epoch 11/100\n",
      "900/900 [==============================] - 0s 438us/step - loss: 0.0232\n",
      "Epoch 12/100\n",
      "900/900 [==============================] - 0s 337us/step - loss: 0.0218\n",
      "Epoch 13/100\n",
      "900/900 [==============================] - 0s 414us/step - loss: 0.0181\n",
      "Epoch 14/100\n",
      "900/900 [==============================] - 0s 357us/step - loss: 0.0157\n",
      "Epoch 15/100\n",
      "900/900 [==============================] - 0s 346us/step - loss: 0.0152\n",
      "Epoch 16/100\n",
      "900/900 [==============================] - 0s 317us/step - loss: 0.0154\n",
      "Epoch 17/100\n",
      "900/900 [==============================] - 0s 326us/step - loss: 0.0202\n",
      "Epoch 18/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0170\n",
      "Epoch 19/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0210\n",
      "Epoch 20/100\n",
      "900/900 [==============================] - 0s 325us/step - loss: 0.0146\n",
      "Epoch 21/100\n",
      "900/900 [==============================] - 0s 326us/step - loss: 0.0136\n",
      "Epoch 22/100\n",
      "900/900 [==============================] - 0s 339us/step - loss: 0.0163\n",
      "Epoch 23/100\n",
      "900/900 [==============================] - 0s 334us/step - loss: 0.0203\n",
      "Epoch 24/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0282\n",
      "Epoch 25/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0280\n",
      "Epoch 26/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0317\n",
      "Epoch 27/100\n",
      "900/900 [==============================] - 0s 316us/step - loss: 0.0244\n",
      "Epoch 28/100\n",
      "900/900 [==============================] - 0s 337us/step - loss: 0.0210\n",
      "Epoch 29/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0196\n",
      "Epoch 30/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0168\n",
      "Epoch 31/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0151\n",
      "Epoch 32/100\n",
      "900/900 [==============================] - 0s 292us/step - loss: 0.0159\n",
      "Epoch 33/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0208\n",
      "Epoch 34/100\n",
      "900/900 [==============================] - 0s 292us/step - loss: 0.0184\n",
      "Epoch 35/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0170\n",
      "Epoch 36/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0189\n",
      "Epoch 37/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0218\n",
      "Epoch 38/100\n",
      "900/900 [==============================] - 0s 349us/step - loss: 0.0457\n",
      "Epoch 39/100\n",
      "900/900 [==============================] - 0s 343us/step - loss: 0.0621\n",
      "Epoch 40/100\n",
      "900/900 [==============================] - 0s 315us/step - loss: 0.0728\n",
      "Epoch 41/100\n",
      "900/900 [==============================] - 0s 341us/step - loss: 0.0559\n",
      "Epoch 42/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0540\n",
      "Epoch 43/100\n",
      "900/900 [==============================] - 0s 345us/step - loss: 0.0452\n",
      "Epoch 44/100\n",
      "900/900 [==============================] - 0s 333us/step - loss: 0.0429\n",
      "Epoch 45/100\n",
      "900/900 [==============================] - 0s 350us/step - loss: 0.0358\n",
      "Epoch 46/100\n",
      "900/900 [==============================] - 0s 339us/step - loss: 0.0301\n",
      "Epoch 47/100\n",
      "900/900 [==============================] - 0s 340us/step - loss: 0.0308\n",
      "Epoch 48/100\n",
      "900/900 [==============================] - 0s 359us/step - loss: 0.0243\n",
      "Epoch 49/100\n",
      "900/900 [==============================] - 0s 343us/step - loss: 0.0267\n",
      "Epoch 50/100\n",
      "900/900 [==============================] - 0s 335us/step - loss: 0.0372\n",
      "Epoch 51/100\n",
      "900/900 [==============================] - 0s 335us/step - loss: 0.0367\n",
      "Epoch 52/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0444\n",
      "Epoch 53/100\n",
      "900/900 [==============================] - 0s 330us/step - loss: 0.0452\n",
      "Epoch 54/100\n",
      "900/900 [==============================] - 0s 355us/step - loss: 0.0516\n",
      "Epoch 55/100\n",
      "900/900 [==============================] - 0s 339us/step - loss: 0.0382\n",
      "Epoch 56/100\n",
      "900/900 [==============================] - 0s 328us/step - loss: 0.0359\n",
      "Epoch 57/100\n",
      "900/900 [==============================] - 0s 335us/step - loss: 0.0303\n",
      "Epoch 58/100\n",
      "900/900 [==============================] - 0s 340us/step - loss: 0.0275\n",
      "Epoch 59/100\n",
      "900/900 [==============================] - 0s 324us/step - loss: 0.0276\n",
      "Epoch 60/100\n",
      "900/900 [==============================] - 0s 326us/step - loss: 0.0243\n",
      "Epoch 61/100\n",
      "900/900 [==============================] - 0s 336us/step - loss: 0.0297\n",
      "Epoch 62/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0256\n",
      "Epoch 63/100\n",
      "900/900 [==============================] - 0s 339us/step - loss: 0.0251\n",
      "Epoch 64/100\n",
      "900/900 [==============================] - 0s 338us/step - loss: 0.0256\n",
      "Epoch 65/100\n",
      "900/900 [==============================] - 0s 336us/step - loss: 0.0205\n",
      "Epoch 66/100\n",
      "900/900 [==============================] - 0s 346us/step - loss: 0.0240\n",
      "Epoch 67/100\n",
      "900/900 [==============================] - 0s 335us/step - loss: 0.0220\n",
      "Epoch 68/100\n",
      "900/900 [==============================] - 0s 337us/step - loss: 0.0186\n",
      "Epoch 69/100\n",
      "900/900 [==============================] - 0s 307us/step - loss: 0.0184\n",
      "Epoch 70/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0194\n",
      "Epoch 71/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0154\n",
      "Epoch 72/100\n",
      "900/900 [==============================] - 0s 327us/step - loss: 0.0170\n",
      "Epoch 73/100\n",
      "900/900 [==============================] - 0s 300us/step - loss: 0.0161\n",
      "Epoch 74/100\n",
      "900/900 [==============================] - 0s 301us/step - loss: 0.0196\n",
      "Epoch 75/100\n",
      "900/900 [==============================] - 0s 307us/step - loss: 0.0160\n",
      "Epoch 76/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0163\n",
      "Epoch 77/100\n",
      "900/900 [==============================] - 0s 307us/step - loss: 0.0157\n",
      "Epoch 78/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0157\n",
      "Epoch 79/100\n",
      "900/900 [==============================] - 0s 328us/step - loss: 0.0153\n",
      "Epoch 80/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0161\n",
      "Epoch 81/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0152\n",
      "Epoch 82/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0164\n",
      "Epoch 83/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      "900/900 [==============================] - 0s 307us/step - loss: 0.0212\n",
      "Epoch 85/100\n",
      "900/900 [==============================] - 0s 320us/step - loss: 0.0215\n",
      "Epoch 86/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0182\n",
      "Epoch 87/100\n",
      "900/900 [==============================] - 0s 342us/step - loss: 0.0187\n",
      "Epoch 88/100\n",
      "900/900 [==============================] - 0s 381us/step - loss: 0.0208\n",
      "Epoch 89/100\n",
      "900/900 [==============================] - 0s 344us/step - loss: 0.0187\n",
      "Epoch 90/100\n",
      "900/900 [==============================] - 0s 320us/step - loss: 0.0239\n",
      "Epoch 91/100\n",
      "900/900 [==============================] - 0s 352us/step - loss: 0.0241\n",
      "Epoch 92/100\n",
      "900/900 [==============================] - 0s 327us/step - loss: 0.0272\n",
      "Epoch 93/100\n",
      "900/900 [==============================] - 0s 335us/step - loss: 0.0294\n",
      "Epoch 94/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0290\n",
      "Epoch 95/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0292\n",
      "Epoch 96/100\n",
      "900/900 [==============================] - 0s 332us/step - loss: 0.0316\n",
      "Epoch 97/100\n",
      "900/900 [==============================] - 0s 319us/step - loss: 0.0313\n",
      "Epoch 98/100\n",
      "900/900 [==============================] - 0s 334us/step - loss: 0.0322\n",
      "Epoch 99/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0298\n",
      "Epoch 100/100\n",
      "900/900 [==============================] - 0s 323us/step - loss: 0.0270\n",
      "100/100 [==============================] - 3s 34ms/step\n",
      "Epoch 1/100\n",
      "900/900 [==============================] - 10s 11ms/step - loss: 6.8053\n",
      "Epoch 2/100\n",
      "900/900 [==============================] - 1s 715us/step - loss: 1.3831\n",
      "Epoch 3/100\n",
      "900/900 [==============================] - 0s 533us/step - loss: 0.5646\n",
      "Epoch 4/100\n",
      "900/900 [==============================] - 0s 448us/step - loss: 0.2255\n",
      "Epoch 5/100\n",
      "900/900 [==============================] - 0s 401us/step - loss: 0.1245\n",
      "Epoch 6/100\n",
      "900/900 [==============================] - 0s 375us/step - loss: 0.0835\n",
      "Epoch 7/100\n",
      "900/900 [==============================] - 0s 381us/step - loss: 0.0573\n",
      "Epoch 8/100\n",
      "900/900 [==============================] - 0s 338us/step - loss: 0.0429\n",
      "Epoch 9/100\n",
      "900/900 [==============================] - 0s 362us/step - loss: 0.0327\n",
      "Epoch 10/100\n",
      "900/900 [==============================] - 0s 315us/step - loss: 0.0274\n",
      "Epoch 11/100\n",
      "900/900 [==============================] - 0s 319us/step - loss: 0.0226\n",
      "Epoch 12/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0272\n",
      "Epoch 13/100\n",
      "900/900 [==============================] - 0s 321us/step - loss: 0.0307\n",
      "Epoch 14/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0234\n",
      "Epoch 15/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0246\n",
      "Epoch 16/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0211\n",
      "Epoch 17/100\n",
      "900/900 [==============================] - 0s 334us/step - loss: 0.0197\n",
      "Epoch 18/100\n",
      "900/900 [==============================] - 0s 315us/step - loss: 0.0185\n",
      "Epoch 19/100\n",
      "900/900 [==============================] - 0s 317us/step - loss: 0.0181\n",
      "Epoch 20/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0171\n",
      "Epoch 21/100\n",
      "900/900 [==============================] - 0s 321us/step - loss: 0.0176\n",
      "Epoch 22/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0204\n",
      "Epoch 23/100\n",
      "900/900 [==============================] - 0s 323us/step - loss: 0.0223\n",
      "Epoch 24/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0207\n",
      "Epoch 25/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0231\n",
      "Epoch 26/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0191\n",
      "Epoch 27/100\n",
      "900/900 [==============================] - 0s 318us/step - loss: 0.0165\n",
      "Epoch 28/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0156\n",
      "Epoch 29/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0164\n",
      "Epoch 30/100\n",
      "900/900 [==============================] - 0s 288us/step - loss: 0.0214\n",
      "Epoch 31/100\n",
      "900/900 [==============================] - 0s 307us/step - loss: 0.0248\n",
      "Epoch 32/100\n",
      "900/900 [==============================] - 0s 324us/step - loss: 0.0351\n",
      "Epoch 33/100\n",
      "900/900 [==============================] - 0s 343us/step - loss: 0.0229\n",
      "Epoch 34/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0172\n",
      "Epoch 35/100\n",
      "900/900 [==============================] - 0s 322us/step - loss: 0.0142\n",
      "Epoch 36/100\n",
      "900/900 [==============================] - 0s 326us/step - loss: 0.0167\n",
      "Epoch 37/100\n",
      "900/900 [==============================] - 0s 344us/step - loss: 0.0173\n",
      "Epoch 38/100\n",
      "900/900 [==============================] - 0s 302us/step - loss: 0.0144\n",
      "Epoch 39/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0134\n",
      "Epoch 40/100\n",
      "900/900 [==============================] - 0s 297us/step - loss: 0.0188\n",
      "Epoch 41/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0162\n",
      "Epoch 42/100\n",
      "900/900 [==============================] - 0s 318us/step - loss: 0.0282\n",
      "Epoch 43/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0311\n",
      "Epoch 44/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0254\n",
      "Epoch 45/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0173\n",
      "Epoch 46/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0147\n",
      "Epoch 47/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0147\n",
      "Epoch 48/100\n",
      "900/900 [==============================] - 0s 312us/step - loss: 0.0180\n",
      "Epoch 49/100\n",
      "900/900 [==============================] - 0s 354us/step - loss: 0.0175\n",
      "Epoch 50/100\n",
      "900/900 [==============================] - 0s 327us/step - loss: 0.0208\n",
      "Epoch 51/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0232\n",
      "Epoch 52/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0225\n",
      "Epoch 53/100\n",
      "900/900 [==============================] - 0s 326us/step - loss: 0.0203\n",
      "Epoch 54/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0204\n",
      "Epoch 55/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0186\n",
      "Epoch 56/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0192\n",
      "Epoch 57/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0215\n",
      "Epoch 58/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0176\n",
      "Epoch 59/100\n",
      "900/900 [==============================] - 0s 302us/step - loss: 0.0167\n",
      "Epoch 60/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0156\n",
      "Epoch 61/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0145\n",
      "Epoch 62/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0150\n",
      "Epoch 63/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0154\n",
      "Epoch 64/100\n",
      "900/900 [==============================] - 0s 302us/step - loss: 0.0169\n",
      "Epoch 65/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0165\n",
      "Epoch 66/100\n",
      "900/900 [==============================] - 0s 319us/step - loss: 0.0165\n",
      "Epoch 67/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0172\n",
      "Epoch 68/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0162\n",
      "Epoch 69/100\n",
      "900/900 [==============================] - 0s 313us/step - loss: 0.0173\n",
      "Epoch 70/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0181\n",
      "Epoch 71/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0180\n",
      "Epoch 72/100\n",
      "900/900 [==============================] - 0s 310us/step - loss: 0.0207\n",
      "Epoch 73/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0199\n",
      "Epoch 74/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0167\n",
      "Epoch 75/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0235\n",
      "Epoch 76/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0232\n",
      "Epoch 77/100\n",
      "900/900 [==============================] - 0s 301us/step - loss: 0.0278\n",
      "Epoch 78/100\n",
      "900/900 [==============================] - 0s 288us/step - loss: 0.0294\n",
      "Epoch 79/100\n",
      "900/900 [==============================] - 0s 281us/step - loss: 0.0304\n",
      "Epoch 80/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0259\n",
      "Epoch 81/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0208\n",
      "Epoch 82/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0203\n",
      "Epoch 83/100\n",
      "900/900 [==============================] - 0s 288us/step - loss: 0.0184\n",
      "Epoch 84/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0181\n",
      "Epoch 85/100\n",
      "900/900 [==============================] - 0s 300us/step - loss: 0.0183\n",
      "Epoch 86/100\n",
      "900/900 [==============================] - 0s 285us/step - loss: 0.0188\n",
      "Epoch 87/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0178\n",
      "Epoch 88/100\n",
      "900/900 [==============================] - 0s 285us/step - loss: 0.0192\n",
      "Epoch 89/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0190\n",
      "Epoch 90/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0158\n",
      "Epoch 91/100\n",
      "900/900 [==============================] - 0s 307us/step - loss: 0.0174\n",
      "Epoch 92/100\n",
      "900/900 [==============================] - 0s 313us/step - loss: 0.0191\n",
      "Epoch 93/100\n",
      "900/900 [==============================] - 0s 349us/step - loss: 0.0204\n",
      "Epoch 94/100\n",
      "900/900 [==============================] - 0s 344us/step - loss: 0.0177\n",
      "Epoch 95/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0174\n",
      "Epoch 96/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0228\n",
      "Epoch 97/100\n",
      "900/900 [==============================] - 0s 279us/step - loss: 0.0232\n",
      "Epoch 98/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0214\n",
      "Epoch 99/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0203\n",
      "Epoch 100/100\n",
      "900/900 [==============================] - 0s 315us/step - loss: 0.0193\n",
      "100/100 [==============================] - 2s 21ms/step\n",
      "Epoch 1/100\n",
      "900/900 [==============================] - 10s 12ms/step - loss: 7.2800\n",
      "Epoch 2/100\n",
      "900/900 [==============================] - 1s 600us/step - loss: 1.4864\n",
      "Epoch 3/100\n",
      "900/900 [==============================] - 1s 558us/step - loss: 0.5836\n",
      "Epoch 4/100\n",
      "900/900 [==============================] - 0s 490us/step - loss: 0.2463\n",
      "Epoch 5/100\n",
      "900/900 [==============================] - 0s 477us/step - loss: 0.1248\n",
      "Epoch 6/100\n",
      "900/900 [==============================] - 0s 416us/step - loss: 0.0879\n",
      "Epoch 7/100\n",
      "900/900 [==============================] - 0s 429us/step - loss: 0.0558\n",
      "Epoch 8/100\n",
      "900/900 [==============================] - 0s 425us/step - loss: 0.0400\n",
      "Epoch 9/100\n",
      "900/900 [==============================] - 0s 362us/step - loss: 0.0324\n",
      "Epoch 10/100\n",
      "900/900 [==============================] - 0s 330us/step - loss: 0.0323\n",
      "Epoch 11/100\n",
      "900/900 [==============================] - 0s 332us/step - loss: 0.0289\n",
      "Epoch 12/100\n",
      "900/900 [==============================] - 0s 324us/step - loss: 0.0271\n",
      "Epoch 13/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0263\n",
      "Epoch 14/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0276\n",
      "Epoch 15/100\n",
      "900/900 [==============================] - 0s 289us/step - loss: 0.0330\n",
      "Epoch 16/100\n",
      "900/900 [==============================] - 0s 276us/step - loss: 0.0269\n",
      "Epoch 17/100\n",
      "900/900 [==============================] - 0s 288us/step - loss: 0.0199\n",
      "Epoch 18/100\n",
      "900/900 [==============================] - 0s 279us/step - loss: 0.0200\n",
      "Epoch 19/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0137\n",
      "Epoch 20/100\n",
      "900/900 [==============================] - 0s 339us/step - loss: 0.0159\n",
      "Epoch 21/100\n",
      "900/900 [==============================] - 0s 328us/step - loss: 0.0255\n",
      "Epoch 22/100\n",
      "900/900 [==============================] - 0s 277us/step - loss: 0.0342\n",
      "Epoch 23/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0329\n",
      "Epoch 24/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0440\n",
      "Epoch 25/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0289\n",
      "Epoch 26/100\n",
      "900/900 [==============================] - 0s 318us/step - loss: 0.0277\n",
      "Epoch 27/100\n",
      "900/900 [==============================] - 0s 278us/step - loss: 0.0329\n",
      "Epoch 28/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0444\n",
      "Epoch 29/100\n",
      "900/900 [==============================] - 0s 276us/step - loss: 0.0451\n",
      "Epoch 30/100\n",
      "900/900 [==============================] - 0s 277us/step - loss: 0.0319\n",
      "Epoch 31/100\n",
      "900/900 [==============================] - 0s 261us/step - loss: 0.0290\n",
      "Epoch 32/100\n",
      "900/900 [==============================] - 0s 302us/step - loss: 0.0216\n",
      "Epoch 33/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0164\n",
      "Epoch 34/100\n",
      "900/900 [==============================] - 0s 277us/step - loss: 0.0168\n",
      "Epoch 35/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0196\n",
      "Epoch 36/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0205\n",
      "Epoch 37/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0156\n",
      "Epoch 38/100\n",
      "900/900 [==============================] - 0s 324us/step - loss: 0.0184\n",
      "Epoch 39/100\n",
      "900/900 [==============================] - 0s 297us/step - loss: 0.0143\n",
      "Epoch 40/100\n",
      "900/900 [==============================] - 0s 277us/step - loss: 0.0118\n",
      "Epoch 41/100\n",
      "900/900 [==============================] - 0s 262us/step - loss: 0.0150\n",
      "Epoch 42/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0176\n",
      "Epoch 43/100\n",
      "900/900 [==============================] - 0s 274us/step - loss: 0.0163\n",
      "Epoch 44/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0189\n",
      "Epoch 45/100\n",
      "900/900 [==============================] - 0s 275us/step - loss: 0.0166\n",
      "Epoch 46/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0175\n",
      "Epoch 47/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0210\n",
      "Epoch 48/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0214\n",
      "Epoch 49/100\n",
      "900/900 [==============================] - 0s 269us/step - loss: 0.0254\n",
      "Epoch 50/100\n",
      "900/900 [==============================] - 0s 288us/step - loss: 0.0209\n",
      "Epoch 51/100\n",
      "900/900 [==============================] - 0s 312us/step - loss: 0.0231\n",
      "Epoch 52/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0325\n",
      "Epoch 53/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0284\n",
      "Epoch 54/100\n",
      "900/900 [==============================] - 0s 301us/step - loss: 0.0260\n",
      "Epoch 55/100\n",
      "900/900 [==============================] - 0s 316us/step - loss: 0.0234\n",
      "Epoch 56/100\n",
      "900/900 [==============================] - 0s 300us/step - loss: 0.0218\n",
      "Epoch 57/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0261\n",
      "Epoch 58/100\n",
      "900/900 [==============================] - 0s 328us/step - loss: 0.0262\n",
      "Epoch 59/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0244\n",
      "Epoch 60/100\n",
      "900/900 [==============================] - 0s 310us/step - loss: 0.0195\n",
      "Epoch 61/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0196\n",
      "Epoch 62/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0240\n",
      "Epoch 63/100\n",
      "900/900 [==============================] - 0s 287us/step - loss: 0.0172\n",
      "Epoch 64/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0163\n",
      "Epoch 65/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0195\n",
      "Epoch 66/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0178\n",
      "Epoch 67/100\n",
      "900/900 [==============================] - 0s 350us/step - loss: 0.0200\n",
      "Epoch 68/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0209\n",
      "Epoch 69/100\n",
      "900/900 [==============================] - 0s 297us/step - loss: 0.0295\n",
      "Epoch 70/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0267\n",
      "Epoch 71/100\n",
      "900/900 [==============================] - 0s 289us/step - loss: 0.0332\n",
      "Epoch 72/100\n",
      "900/900 [==============================] - 0s 277us/step - loss: 0.0295\n",
      "Epoch 73/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0313\n",
      "Epoch 74/100\n",
      "900/900 [==============================] - 0s 289us/step - loss: 0.0294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0308\n",
      "Epoch 76/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0301\n",
      "Epoch 77/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0289\n",
      "Epoch 78/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0318\n",
      "Epoch 79/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0266\n",
      "Epoch 80/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0252\n",
      "Epoch 81/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0245\n",
      "Epoch 82/100\n",
      "900/900 [==============================] - 0s 282us/step - loss: 0.0256\n",
      "Epoch 83/100\n",
      "900/900 [==============================] - 0s 300us/step - loss: 0.0232\n",
      "Epoch 84/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0236\n",
      "Epoch 85/100\n",
      "900/900 [==============================] - 0s 301us/step - loss: 0.0279\n",
      "Epoch 86/100\n",
      "900/900 [==============================] - 0s 346us/step - loss: 0.0298\n",
      "Epoch 87/100\n",
      "900/900 [==============================] - 0s 332us/step - loss: 0.0261\n",
      "Epoch 88/100\n",
      "900/900 [==============================] - 0s 344us/step - loss: 0.0252\n",
      "Epoch 89/100\n",
      "900/900 [==============================] - 0s 335us/step - loss: 0.0241\n",
      "Epoch 90/100\n",
      "900/900 [==============================] - 0s 332us/step - loss: 0.0218\n",
      "Epoch 91/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0187\n",
      "Epoch 92/100\n",
      "900/900 [==============================] - 0s 297us/step - loss: 0.0165\n",
      "Epoch 93/100\n",
      "900/900 [==============================] - 0s 316us/step - loss: 0.0164\n",
      "Epoch 94/100\n",
      "900/900 [==============================] - 0s 325us/step - loss: 0.0159\n",
      "Epoch 95/100\n",
      "900/900 [==============================] - 0s 348us/step - loss: 0.0139\n",
      "Epoch 96/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0154\n",
      "Epoch 97/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0150\n",
      "Epoch 98/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0180\n",
      "Epoch 99/100\n",
      "900/900 [==============================] - 0s 320us/step - loss: 0.0204\n",
      "Epoch 100/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0271\n",
      "100/100 [==============================] - 2s 24ms/step\n",
      "Epoch 1/100\n",
      "900/900 [==============================] - 10s 11ms/step - loss: 4.7713\n",
      "Epoch 2/100\n",
      "900/900 [==============================] - 1s 586us/step - loss: 1.1793\n",
      "Epoch 3/100\n",
      "900/900 [==============================] - 0s 537us/step - loss: 0.5311\n",
      "Epoch 4/100\n",
      "900/900 [==============================] - 0s 447us/step - loss: 0.2765\n",
      "Epoch 5/100\n",
      "900/900 [==============================] - 0s 474us/step - loss: 0.1894\n",
      "Epoch 6/100\n",
      "900/900 [==============================] - 0s 422us/step - loss: 0.1478\n",
      "Epoch 7/100\n",
      "900/900 [==============================] - 0s 444us/step - loss: 0.1057\n",
      "Epoch 8/100\n",
      "900/900 [==============================] - 0s 332us/step - loss: 0.0912\n",
      "Epoch 9/100\n",
      "900/900 [==============================] - 0s 367us/step - loss: 0.0532\n",
      "Epoch 10/100\n",
      "900/900 [==============================] - 0s 344us/step - loss: 0.0339\n",
      "Epoch 11/100\n",
      "900/900 [==============================] - 0s 312us/step - loss: 0.0299\n",
      "Epoch 12/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0272\n",
      "Epoch 13/100\n",
      "900/900 [==============================] - 0s 320us/step - loss: 0.0265\n",
      "Epoch 14/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0234\n",
      "Epoch 15/100\n",
      "900/900 [==============================] - 0s 301us/step - loss: 0.0236\n",
      "Epoch 16/100\n",
      "900/900 [==============================] - 0s 289us/step - loss: 0.0195\n",
      "Epoch 17/100\n",
      "900/900 [==============================] - 0s 273us/step - loss: 0.0222\n",
      "Epoch 18/100\n",
      "900/900 [==============================] - 0s 280us/step - loss: 0.0178\n",
      "Epoch 19/100\n",
      "900/900 [==============================] - 0s 272us/step - loss: 0.0193\n",
      "Epoch 20/100\n",
      "900/900 [==============================] - 0s 262us/step - loss: 0.0197\n",
      "Epoch 21/100\n",
      "900/900 [==============================] - 0s 265us/step - loss: 0.0202\n",
      "Epoch 22/100\n",
      "900/900 [==============================] - 0s 263us/step - loss: 0.0182\n",
      "Epoch 23/100\n",
      "900/900 [==============================] - 0s 279us/step - loss: 0.0252\n",
      "Epoch 24/100\n",
      "900/900 [==============================] - 0s 268us/step - loss: 0.0280\n",
      "Epoch 25/100\n",
      "900/900 [==============================] - 0s 264us/step - loss: 0.0184\n",
      "Epoch 26/100\n",
      "900/900 [==============================] - 0s 268us/step - loss: 0.0163\n",
      "Epoch 27/100\n",
      "900/900 [==============================] - 0s 277us/step - loss: 0.0163\n",
      "Epoch 28/100\n",
      "900/900 [==============================] - 0s 257us/step - loss: 0.0130\n",
      "Epoch 29/100\n",
      "900/900 [==============================] - 0s 268us/step - loss: 0.0145\n",
      "Epoch 30/100\n",
      "900/900 [==============================] - 0s 272us/step - loss: 0.0112\n",
      "Epoch 31/100\n",
      "900/900 [==============================] - 0s 258us/step - loss: 0.0128\n",
      "Epoch 32/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0155\n",
      "Epoch 33/100\n",
      "900/900 [==============================] - 0s 265us/step - loss: 0.0133\n",
      "Epoch 34/100\n",
      "900/900 [==============================] - 0s 266us/step - loss: 0.0230\n",
      "Epoch 35/100\n",
      "900/900 [==============================] - 0s 263us/step - loss: 0.0247\n",
      "Epoch 36/100\n",
      "900/900 [==============================] - 0s 273us/step - loss: 0.0257\n",
      "Epoch 37/100\n",
      "900/900 [==============================] - 0s 270us/step - loss: 0.0260\n",
      "Epoch 38/100\n",
      "900/900 [==============================] - 0s 273us/step - loss: 0.0224\n",
      "Epoch 39/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0235\n",
      "Epoch 40/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0230\n",
      "Epoch 41/100\n",
      "900/900 [==============================] - 0s 277us/step - loss: 0.0328\n",
      "Epoch 42/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0302\n",
      "Epoch 43/100\n",
      "900/900 [==============================] - 0s 278us/step - loss: 0.0279\n",
      "Epoch 44/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0243\n",
      "Epoch 45/100\n",
      "900/900 [==============================] - 0s 270us/step - loss: 0.0212\n",
      "Epoch 46/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0173\n",
      "Epoch 47/100\n",
      "900/900 [==============================] - 0s 273us/step - loss: 0.0170\n",
      "Epoch 48/100\n",
      "900/900 [==============================] - 0s 272us/step - loss: 0.0179\n",
      "Epoch 49/100\n",
      "900/900 [==============================] - 0s 276us/step - loss: 0.0174\n",
      "Epoch 50/100\n",
      "900/900 [==============================] - 0s 273us/step - loss: 0.0212\n",
      "Epoch 51/100\n",
      "900/900 [==============================] - 0s 271us/step - loss: 0.0258\n",
      "Epoch 52/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0187\n",
      "Epoch 53/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0198\n",
      "Epoch 54/100\n",
      "900/900 [==============================] - 0s 274us/step - loss: 0.0222\n",
      "Epoch 55/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0287\n",
      "Epoch 56/100\n",
      "900/900 [==============================] - 0s 280us/step - loss: 0.0257\n",
      "Epoch 57/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0355\n",
      "Epoch 58/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0310\n",
      "Epoch 59/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0442\n",
      "Epoch 60/100\n",
      "900/900 [==============================] - 0s 263us/step - loss: 0.0453\n",
      "Epoch 61/100\n",
      "900/900 [==============================] - 0s 277us/step - loss: 0.0519\n",
      "Epoch 62/100\n",
      "900/900 [==============================] - 0s 280us/step - loss: 0.0463\n",
      "Epoch 63/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0428\n",
      "Epoch 64/100\n",
      "900/900 [==============================] - 0s 285us/step - loss: 0.0328\n",
      "Epoch 65/100\n",
      "900/900 [==============================] - 0s 278us/step - loss: 0.0307\n",
      "Epoch 66/100\n",
      "900/900 [==============================] - 0s 272us/step - loss: 0.0246\n",
      "Epoch 67/100\n",
      "900/900 [==============================] - 0s 278us/step - loss: 0.0259\n",
      "Epoch 68/100\n",
      "900/900 [==============================] - 0s 275us/step - loss: 0.0288\n",
      "Epoch 69/100\n",
      "900/900 [==============================] - 0s 267us/step - loss: 0.0343\n",
      "Epoch 70/100\n",
      "900/900 [==============================] - 0s 263us/step - loss: 0.0291\n",
      "Epoch 71/100\n",
      "900/900 [==============================] - 0s 271us/step - loss: 0.0328\n",
      "Epoch 72/100\n",
      "900/900 [==============================] - 0s 273us/step - loss: 0.0336\n",
      "Epoch 73/100\n",
      "900/900 [==============================] - 0s 292us/step - loss: 0.0312\n",
      "Epoch 74/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0265\n",
      "Epoch 75/100\n",
      "900/900 [==============================] - 0s 317us/step - loss: 0.0249\n",
      "Epoch 76/100\n",
      "900/900 [==============================] - 0s 315us/step - loss: 0.0306\n",
      "Epoch 77/100\n",
      "900/900 [==============================] - 0s 364us/step - loss: 0.0291\n",
      "Epoch 78/100\n",
      "900/900 [==============================] - 0s 364us/step - loss: 0.0243\n",
      "Epoch 79/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0225\n",
      "Epoch 80/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0213\n",
      "Epoch 81/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0207\n",
      "Epoch 82/100\n",
      "900/900 [==============================] - 0s 355us/step - loss: 0.0219\n",
      "Epoch 83/100\n",
      "900/900 [==============================] - 0s 384us/step - loss: 0.0207\n",
      "Epoch 84/100\n",
      "900/900 [==============================] - 0s 399us/step - loss: 0.0215\n",
      "Epoch 85/100\n",
      "900/900 [==============================] - 0s 369us/step - loss: 0.0223\n",
      "Epoch 86/100\n",
      "900/900 [==============================] - 0s 323us/step - loss: 0.0229\n",
      "Epoch 87/100\n",
      "900/900 [==============================] - 0s 341us/step - loss: 0.0195\n",
      "Epoch 88/100\n",
      "900/900 [==============================] - 0s 318us/step - loss: 0.0221\n",
      "Epoch 89/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0186\n",
      "Epoch 90/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0192\n",
      "Epoch 91/100\n",
      "900/900 [==============================] - 0s 285us/step - loss: 0.0209\n",
      "Epoch 92/100\n",
      "900/900 [==============================] - 0s 327us/step - loss: 0.0232\n",
      "Epoch 93/100\n",
      "900/900 [==============================] - 0s 330us/step - loss: 0.0196\n",
      "Epoch 94/100\n",
      "900/900 [==============================] - 0s 324us/step - loss: 0.0206\n",
      "Epoch 95/100\n",
      "900/900 [==============================] - 0s 346us/step - loss: 0.0164\n",
      "Epoch 96/100\n",
      "900/900 [==============================] - 0s 325us/step - loss: 0.0185\n",
      "Epoch 97/100\n",
      "900/900 [==============================] - 0s 324us/step - loss: 0.0144\n",
      "Epoch 98/100\n",
      "900/900 [==============================] - 0s 316us/step - loss: 0.0210\n",
      "Epoch 99/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0254\n",
      "Epoch 100/100\n",
      "900/900 [==============================] - 0s 336us/step - loss: 0.0325\n",
      "100/100 [==============================] - 2s 20ms/step\n",
      "Epoch 1/100\n",
      "900/900 [==============================] - 11s 13ms/step - loss: 7.1692\n",
      "Epoch 2/100\n",
      "900/900 [==============================] - 1s 655us/step - loss: 1.5802\n",
      "Epoch 3/100\n",
      "900/900 [==============================] - 0s 552us/step - loss: 0.6528\n",
      "Epoch 4/100\n",
      "900/900 [==============================] - 0s 490us/step - loss: 0.2972\n",
      "Epoch 5/100\n",
      "900/900 [==============================] - 0s 449us/step - loss: 0.1539\n",
      "Epoch 6/100\n",
      "900/900 [==============================] - 0s 405us/step - loss: 0.0941\n",
      "Epoch 7/100\n",
      "900/900 [==============================] - 0s 378us/step - loss: 0.0616\n",
      "Epoch 8/100\n",
      "900/900 [==============================] - 0s 345us/step - loss: 0.0436\n",
      "Epoch 9/100\n",
      "900/900 [==============================] - 0s 356us/step - loss: 0.0338\n",
      "Epoch 10/100\n",
      "900/900 [==============================] - 0s 318us/step - loss: 0.0260\n",
      "Epoch 11/100\n",
      "900/900 [==============================] - 0s 327us/step - loss: 0.0234\n",
      "Epoch 12/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0209\n",
      "Epoch 13/100\n",
      "900/900 [==============================] - 0s 324us/step - loss: 0.0320\n",
      "Epoch 14/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0255\n",
      "Epoch 15/100\n",
      "900/900 [==============================] - 0s 289us/step - loss: 0.0222\n",
      "Epoch 16/100\n",
      "900/900 [==============================] - 0s 302us/step - loss: 0.0177\n",
      "Epoch 17/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0218\n",
      "Epoch 18/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0191\n",
      "Epoch 19/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0140\n",
      "Epoch 20/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0142\n",
      "Epoch 21/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0169\n",
      "Epoch 22/100\n",
      "900/900 [==============================] - 0s 287us/step - loss: 0.0171\n",
      "Epoch 23/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0157\n",
      "Epoch 24/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0169\n",
      "Epoch 25/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0152\n",
      "Epoch 26/100\n",
      "900/900 [==============================] - 0s 328us/step - loss: 0.0137\n",
      "Epoch 27/100\n",
      "900/900 [==============================] - 0s 326us/step - loss: 0.0224\n",
      "Epoch 28/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0176\n",
      "Epoch 29/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0115\n",
      "Epoch 30/100\n",
      "900/900 [==============================] - 0s 297us/step - loss: 0.0110\n",
      "Epoch 31/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0121\n",
      "Epoch 32/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0128\n",
      "Epoch 33/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0112\n",
      "Epoch 34/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0112\n",
      "Epoch 35/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0111\n",
      "Epoch 36/100\n",
      "900/900 [==============================] - 0s 307us/step - loss: 0.0102\n",
      "Epoch 37/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0108\n",
      "Epoch 38/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0094\n",
      "Epoch 39/100\n",
      "900/900 [==============================] - 0s 347us/step - loss: 0.0098\n",
      "Epoch 40/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0086\n",
      "Epoch 41/100\n",
      "900/900 [==============================] - 0s 334us/step - loss: 0.0160\n",
      "Epoch 42/100\n",
      "900/900 [==============================] - 0s 327us/step - loss: 0.0172\n",
      "Epoch 43/100\n",
      "900/900 [==============================] - 0s 307us/step - loss: 0.0164\n",
      "Epoch 44/100\n",
      "900/900 [==============================] - 0s 315us/step - loss: 0.0130\n",
      "Epoch 45/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0100\n",
      "Epoch 46/100\n",
      "900/900 [==============================] - 0s 315us/step - loss: 0.0149\n",
      "Epoch 47/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0354\n",
      "Epoch 48/100\n",
      "900/900 [==============================] - 0s 316us/step - loss: 0.0282\n",
      "Epoch 49/100\n",
      "900/900 [==============================] - 0s 316us/step - loss: 0.0188\n",
      "Epoch 50/100\n",
      "900/900 [==============================] - 0s 300us/step - loss: 0.0161\n",
      "Epoch 51/100\n",
      "900/900 [==============================] - 0s 288us/step - loss: 0.0300\n",
      "Epoch 52/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0328\n",
      "Epoch 53/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0269\n",
      "Epoch 54/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0245\n",
      "Epoch 55/100\n",
      "900/900 [==============================] - 0s 282us/step - loss: 0.0187\n",
      "Epoch 56/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0186\n",
      "Epoch 57/100\n",
      "900/900 [==============================] - 0s 287us/step - loss: 0.0166\n",
      "Epoch 58/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0154\n",
      "Epoch 59/100\n",
      "900/900 [==============================] - 0s 318us/step - loss: 0.0172\n",
      "Epoch 60/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0168\n",
      "Epoch 61/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0204\n",
      "Epoch 62/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0246\n",
      "Epoch 63/100\n",
      "900/900 [==============================] - 0s 300us/step - loss: 0.0221\n",
      "Epoch 64/100\n",
      "900/900 [==============================] - 0s 292us/step - loss: 0.0410\n",
      "Epoch 65/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      "900/900 [==============================] - 0s 333us/step - loss: 0.0346\n",
      "Epoch 67/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0267\n",
      "Epoch 68/100\n",
      "900/900 [==============================] - 0s 356us/step - loss: 0.0280\n",
      "Epoch 69/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0240\n",
      "Epoch 70/100\n",
      "900/900 [==============================] - 0s 316us/step - loss: 0.0203\n",
      "Epoch 71/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0166\n",
      "Epoch 72/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0124\n",
      "Epoch 73/100\n",
      "900/900 [==============================] - 0s 318us/step - loss: 0.0163\n",
      "Epoch 74/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0133\n",
      "Epoch 75/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0118\n",
      "Epoch 76/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0142\n",
      "Epoch 77/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0113\n",
      "Epoch 78/100\n",
      "900/900 [==============================] - 0s 301us/step - loss: 0.0106\n",
      "Epoch 79/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0097\n",
      "Epoch 80/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0107\n",
      "Epoch 81/100\n",
      "900/900 [==============================] - 0s 312us/step - loss: 0.0117\n",
      "Epoch 82/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0155\n",
      "Epoch 83/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0182 0s - loss: 0.01\n",
      "Epoch 84/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0149\n",
      "Epoch 85/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0205\n",
      "Epoch 86/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0146 0s - loss: 0.0\n",
      "Epoch 87/100\n",
      "900/900 [==============================] - 0s 300us/step - loss: 0.0128\n",
      "Epoch 88/100\n",
      "900/900 [==============================] - 0s 302us/step - loss: 0.0145\n",
      "Epoch 89/100\n",
      "900/900 [==============================] - 0s 307us/step - loss: 0.0124\n",
      "Epoch 90/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0152\n",
      "Epoch 91/100\n",
      "900/900 [==============================] - 0s 301us/step - loss: 0.0181\n",
      "Epoch 92/100\n",
      "900/900 [==============================] - 0s 319us/step - loss: 0.0199\n",
      "Epoch 93/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0278\n",
      "Epoch 94/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0220\n",
      "Epoch 95/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0220\n",
      "Epoch 96/100\n",
      "900/900 [==============================] - 0s 316us/step - loss: 0.0177\n",
      "Epoch 97/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0161\n",
      "Epoch 98/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0171\n",
      "Epoch 99/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0142\n",
      "Epoch 100/100\n",
      "900/900 [==============================] - 0s 321us/step - loss: 0.0128\n",
      "100/100 [==============================] - 3s 28ms/step\n",
      "Epoch 1/100\n",
      "900/900 [==============================] - 9s 10ms/step - loss: 7.0788\n",
      "Epoch 2/100\n",
      "900/900 [==============================] - 0s 407us/step - loss: 1.4997\n",
      "Epoch 3/100\n",
      "900/900 [==============================] - 0s 413us/step - loss: 0.7026\n",
      "Epoch 4/100\n",
      "900/900 [==============================] - 0s 396us/step - loss: 0.3346\n",
      "Epoch 5/100\n",
      "900/900 [==============================] - 0s 437us/step - loss: 0.1942\n",
      "Epoch 6/100\n",
      "900/900 [==============================] - 0s 364us/step - loss: 0.1075\n",
      "Epoch 7/100\n",
      "900/900 [==============================] - 0s 346us/step - loss: 0.0627\n",
      "Epoch 8/100\n",
      "900/900 [==============================] - 0s 336us/step - loss: 0.0450\n",
      "Epoch 9/100\n",
      "900/900 [==============================] - 0s 334us/step - loss: 0.0351\n",
      "Epoch 10/100\n",
      "900/900 [==============================] - 0s 360us/step - loss: 0.0271\n",
      "Epoch 11/100\n",
      "900/900 [==============================] - 0s 350us/step - loss: 0.0325\n",
      "Epoch 12/100\n",
      "900/900 [==============================] - 0s 324us/step - loss: 0.0303\n",
      "Epoch 13/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0274\n",
      "Epoch 14/100\n",
      "900/900 [==============================] - 0s 328us/step - loss: 0.0215\n",
      "Epoch 15/100\n",
      "900/900 [==============================] - 0s 330us/step - loss: 0.0191\n",
      "Epoch 16/100\n",
      "900/900 [==============================] - 0s 407us/step - loss: 0.0152\n",
      "Epoch 17/100\n",
      "900/900 [==============================] - 0s 332us/step - loss: 0.0201\n",
      "Epoch 18/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0204\n",
      "Epoch 19/100\n",
      "900/900 [==============================] - 0s 313us/step - loss: 0.0195\n",
      "Epoch 20/100\n",
      "900/900 [==============================] - 0s 287us/step - loss: 0.0331\n",
      "Epoch 21/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0261\n",
      "Epoch 22/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0241\n",
      "Epoch 23/100\n",
      "900/900 [==============================] - 0s 292us/step - loss: 0.0180\n",
      "Epoch 24/100\n",
      "900/900 [==============================] - 0s 297us/step - loss: 0.0192\n",
      "Epoch 25/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0138\n",
      "Epoch 26/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0148\n",
      "Epoch 27/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0126\n",
      "Epoch 28/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0114\n",
      "Epoch 29/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0151\n",
      "Epoch 30/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0176\n",
      "Epoch 31/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0182\n",
      "Epoch 32/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0238\n",
      "Epoch 33/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0339\n",
      "Epoch 34/100\n",
      "900/900 [==============================] - 0s 319us/step - loss: 0.0331\n",
      "Epoch 35/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0235\n",
      "Epoch 36/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0214\n",
      "Epoch 37/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0308\n",
      "Epoch 38/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0375\n",
      "Epoch 39/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0261\n",
      "Epoch 40/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0180\n",
      "Epoch 41/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0236\n",
      "Epoch 42/100\n",
      "900/900 [==============================] - 0s 292us/step - loss: 0.0173\n",
      "Epoch 43/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0179\n",
      "Epoch 44/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0160\n",
      "Epoch 45/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0182\n",
      "Epoch 46/100\n",
      "900/900 [==============================] - 0s 307us/step - loss: 0.0229\n",
      "Epoch 47/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0212\n",
      "Epoch 48/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0314\n",
      "Epoch 49/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0207\n",
      "Epoch 50/100\n",
      "900/900 [==============================] - 0s 296us/step - loss: 0.0204\n",
      "Epoch 51/100\n",
      "900/900 [==============================] - 0s 312us/step - loss: 0.0189\n",
      "Epoch 52/100\n",
      "900/900 [==============================] - 0s 302us/step - loss: 0.0183\n",
      "Epoch 53/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0201\n",
      "Epoch 54/100\n",
      "900/900 [==============================] - 0s 290us/step - loss: 0.0260\n",
      "Epoch 55/100\n",
      "900/900 [==============================] - 0s 297us/step - loss: 0.0311\n",
      "Epoch 56/100\n",
      "900/900 [==============================] - 0s 297us/step - loss: 0.0272\n",
      "Epoch 57/100\n",
      "900/900 [==============================] - 0s 316us/step - loss: 0.0194\n",
      "Epoch 58/100\n",
      "900/900 [==============================] - 0s 325us/step - loss: 0.0226\n",
      "Epoch 59/100\n",
      "900/900 [==============================] - 0s 309us/step - loss: 0.0279\n",
      "Epoch 60/100\n",
      "900/900 [==============================] - 0s 313us/step - loss: 0.0274\n",
      "Epoch 61/100\n",
      "900/900 [==============================] - 0s 330us/step - loss: 0.0233\n",
      "Epoch 62/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0224\n",
      "Epoch 63/100\n",
      "900/900 [==============================] - 0s 321us/step - loss: 0.0142\n",
      "Epoch 64/100\n",
      "900/900 [==============================] - 0s 322us/step - loss: 0.0157\n",
      "Epoch 65/100\n",
      "900/900 [==============================] - 0s 340us/step - loss: 0.0208\n",
      "Epoch 66/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0262\n",
      "Epoch 67/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0241\n",
      "Epoch 68/100\n",
      "900/900 [==============================] - 0s 287us/step - loss: 0.0258\n",
      "Epoch 69/100\n",
      "900/900 [==============================] - 0s 289us/step - loss: 0.0263\n",
      "Epoch 70/100\n",
      "900/900 [==============================] - 0s 322us/step - loss: 0.0194\n",
      "Epoch 71/100\n",
      "900/900 [==============================] - 0s 329us/step - loss: 0.0271\n",
      "Epoch 72/100\n",
      "900/900 [==============================] - 0s 273us/step - loss: 0.0231\n",
      "Epoch 73/100\n",
      "900/900 [==============================] - 0s 262us/step - loss: 0.0154\n",
      "Epoch 74/100\n",
      "900/900 [==============================] - 0s 280us/step - loss: 0.0180\n",
      "Epoch 75/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0148\n",
      "Epoch 76/100\n",
      "900/900 [==============================] - 0s 277us/step - loss: 0.0171\n",
      "Epoch 77/100\n",
      "900/900 [==============================] - 0s 274us/step - loss: 0.0134\n",
      "Epoch 78/100\n",
      "900/900 [==============================] - 0s 275us/step - loss: 0.0157\n",
      "Epoch 79/100\n",
      "900/900 [==============================] - 0s 281us/step - loss: 0.0117\n",
      "Epoch 80/100\n",
      "900/900 [==============================] - 0s 294us/step - loss: 0.0110\n",
      "Epoch 81/100\n",
      "900/900 [==============================] - 0s 277us/step - loss: 0.0160\n",
      "Epoch 82/100\n",
      "900/900 [==============================] - 0s 278us/step - loss: 0.0180\n",
      "Epoch 83/100\n",
      "900/900 [==============================] - 0s 274us/step - loss: 0.0152\n",
      "Epoch 84/100\n",
      "900/900 [==============================] - 0s 277us/step - loss: 0.0197\n",
      "Epoch 85/100\n",
      "900/900 [==============================] - 0s 286us/step - loss: 0.0233\n",
      "Epoch 86/100\n",
      "900/900 [==============================] - 0s 281us/step - loss: 0.0199\n",
      "Epoch 87/100\n",
      "900/900 [==============================] - 0s 288us/step - loss: 0.0141\n",
      "Epoch 88/100\n",
      "900/900 [==============================] - 0s 283us/step - loss: 0.0116\n",
      "Epoch 89/100\n",
      "900/900 [==============================] - 0s 278us/step - loss: 0.0110\n",
      "Epoch 90/100\n",
      "900/900 [==============================] - 0s 279us/step - loss: 0.0128\n",
      "Epoch 91/100\n",
      "900/900 [==============================] - 0s 274us/step - loss: 0.0129\n",
      "Epoch 92/100\n",
      "900/900 [==============================] - 0s 285us/step - loss: 0.0149\n",
      "Epoch 93/100\n",
      "900/900 [==============================] - 0s 275us/step - loss: 0.0163\n",
      "Epoch 94/100\n",
      "900/900 [==============================] - 0s 281us/step - loss: 0.0148\n",
      "Epoch 95/100\n",
      "900/900 [==============================] - 0s 274us/step - loss: 0.0254\n",
      "Epoch 96/100\n",
      "900/900 [==============================] - 0s 291us/step - loss: 0.0261\n",
      "Epoch 97/100\n",
      "900/900 [==============================] - 0s 273us/step - loss: 0.0197\n",
      "Epoch 98/100\n",
      "900/900 [==============================] - 0s 268us/step - loss: 0.0235\n",
      "Epoch 99/100\n",
      "900/900 [==============================] - 0s 282us/step - loss: 0.0199\n",
      "Epoch 100/100\n",
      "900/900 [==============================] - 0s 284us/step - loss: 0.0236\n",
      "100/100 [==============================] - 4s 37ms/step\n",
      "Epoch 1/100\n",
      "900/900 [==============================] - 11s 12ms/step - loss: 5.2982\n",
      "Epoch 2/100\n",
      "900/900 [==============================] - 1s 578us/step - loss: 1.1795\n",
      "Epoch 3/100\n",
      "900/900 [==============================] - 0s 496us/step - loss: 0.4819\n",
      "Epoch 4/100\n",
      "900/900 [==============================] - 0s 433us/step - loss: 0.2019\n",
      "Epoch 5/100\n",
      "900/900 [==============================] - 0s 404us/step - loss: 0.1035\n",
      "Epoch 6/100\n",
      "900/900 [==============================] - 0s 448us/step - loss: 0.0675\n",
      "Epoch 7/100\n",
      "900/900 [==============================] - 0s 370us/step - loss: 0.0629\n",
      "Epoch 8/100\n",
      "900/900 [==============================] - 0s 360us/step - loss: 0.0623\n",
      "Epoch 9/100\n",
      "900/900 [==============================] - 0s 329us/step - loss: 0.0487\n",
      "Epoch 10/100\n",
      "900/900 [==============================] - 0s 323us/step - loss: 0.0448\n",
      "Epoch 11/100\n",
      "900/900 [==============================] - 0s 352us/step - loss: 0.0383\n",
      "Epoch 12/100\n",
      "900/900 [==============================] - 0s 312us/step - loss: 0.0318\n",
      "Epoch 13/100\n",
      "900/900 [==============================] - 0s 317us/step - loss: 0.0252\n",
      "Epoch 14/100\n",
      "900/900 [==============================] - 0s 359us/step - loss: 0.0261\n",
      "Epoch 15/100\n",
      "900/900 [==============================] - 0s 358us/step - loss: 0.0258\n",
      "Epoch 16/100\n",
      "900/900 [==============================] - 0s 329us/step - loss: 0.0242\n",
      "Epoch 17/100\n",
      "900/900 [==============================] - 0s 334us/step - loss: 0.0232\n",
      "Epoch 18/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0190\n",
      "Epoch 19/100\n",
      "900/900 [==============================] - 0s 332us/step - loss: 0.0161\n",
      "Epoch 20/100\n",
      "900/900 [==============================] - 0s 293us/step - loss: 0.0157\n",
      "Epoch 21/100\n",
      "900/900 [==============================] - 0s 287us/step - loss: 0.0203\n",
      "Epoch 22/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0156\n",
      "Epoch 23/100\n",
      "900/900 [==============================] - 0s 322us/step - loss: 0.0234\n",
      "Epoch 24/100\n",
      "900/900 [==============================] - 0s 346us/step - loss: 0.0231\n",
      "Epoch 25/100\n",
      "900/900 [==============================] - 0s 326us/step - loss: 0.0179\n",
      "Epoch 26/100\n",
      "900/900 [==============================] - 0s 345us/step - loss: 0.0164\n",
      "Epoch 27/100\n",
      "900/900 [==============================] - 0s 375us/step - loss: 0.0238\n",
      "Epoch 28/100\n",
      "900/900 [==============================] - 0s 324us/step - loss: 0.0280\n",
      "Epoch 29/100\n",
      "900/900 [==============================] - 0s 327us/step - loss: 0.0213\n",
      "Epoch 30/100\n",
      "900/900 [==============================] - 0s 337us/step - loss: 0.0213\n",
      "Epoch 31/100\n",
      "900/900 [==============================] - 0s 349us/step - loss: 0.0167\n",
      "Epoch 32/100\n",
      "900/900 [==============================] - 0s 376us/step - loss: 0.0148\n",
      "Epoch 33/100\n",
      "900/900 [==============================] - 0s 337us/step - loss: 0.0148\n",
      "Epoch 34/100\n",
      "900/900 [==============================] - 0s 332us/step - loss: 0.0153\n",
      "Epoch 35/100\n",
      "900/900 [==============================] - 0s 352us/step - loss: 0.0142\n",
      "Epoch 36/100\n",
      "900/900 [==============================] - 0s 342us/step - loss: 0.0156\n",
      "Epoch 37/100\n",
      "900/900 [==============================] - 0s 312us/step - loss: 0.0128\n",
      "Epoch 38/100\n",
      "900/900 [==============================] - 0s 316us/step - loss: 0.0146\n",
      "Epoch 39/100\n",
      "900/900 [==============================] - 0s 329us/step - loss: 0.0191\n",
      "Epoch 40/100\n",
      "900/900 [==============================] - 0s 301us/step - loss: 0.0132\n",
      "Epoch 41/100\n",
      "900/900 [==============================] - 0s 323us/step - loss: 0.0130\n",
      "Epoch 42/100\n",
      "900/900 [==============================] - 0s 303us/step - loss: 0.0135\n",
      "Epoch 43/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0169\n",
      "Epoch 44/100\n",
      "900/900 [==============================] - 0s 332us/step - loss: 0.0161\n",
      "Epoch 45/100\n",
      "900/900 [==============================] - 0s 316us/step - loss: 0.0198\n",
      "Epoch 46/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0190\n",
      "Epoch 47/100\n",
      "900/900 [==============================] - 0s 304us/step - loss: 0.0296\n",
      "Epoch 48/100\n",
      "900/900 [==============================] - 0s 307us/step - loss: 0.0305\n",
      "Epoch 49/100\n",
      "900/900 [==============================] - 0s 328us/step - loss: 0.0214\n",
      "Epoch 50/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0212\n",
      "Epoch 51/100\n",
      "900/900 [==============================] - 0s 330us/step - loss: 0.0241\n",
      "Epoch 52/100\n",
      "900/900 [==============================] - 0s 318us/step - loss: 0.0323\n",
      "Epoch 53/100\n",
      "900/900 [==============================] - 0s 307us/step - loss: 0.0278\n",
      "Epoch 54/100\n",
      "900/900 [==============================] - 0s 327us/step - loss: 0.0206\n",
      "Epoch 55/100\n",
      "900/900 [==============================] - 0s 295us/step - loss: 0.0212\n",
      "Epoch 56/100\n",
      "900/900 [==============================] - 0s 289us/step - loss: 0.0267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "900/900 [==============================] - 0s 321us/step - loss: 0.0283\n",
      "Epoch 58/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0283\n",
      "Epoch 59/100\n",
      "900/900 [==============================] - 0s 337us/step - loss: 0.0282\n",
      "Epoch 60/100\n",
      "900/900 [==============================] - 0s 330us/step - loss: 0.0273\n",
      "Epoch 61/100\n",
      "900/900 [==============================] - 0s 326us/step - loss: 0.0235\n",
      "Epoch 62/100\n",
      "900/900 [==============================] - 0s 329us/step - loss: 0.0296\n",
      "Epoch 63/100\n",
      "900/900 [==============================] - 0s 322us/step - loss: 0.0310\n",
      "Epoch 64/100\n",
      "900/900 [==============================] - 0s 316us/step - loss: 0.0296\n",
      "Epoch 65/100\n",
      "900/900 [==============================] - 0s 321us/step - loss: 0.0265\n",
      "Epoch 66/100\n",
      "900/900 [==============================] - 0s 336us/step - loss: 0.0280\n",
      "Epoch 67/100\n",
      "900/900 [==============================] - 0s 320us/step - loss: 0.0314\n",
      "Epoch 68/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0390\n",
      "Epoch 69/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0402\n",
      "Epoch 70/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0296\n",
      "Epoch 71/100\n",
      "900/900 [==============================] - 0s 330us/step - loss: 0.0251\n",
      "Epoch 72/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0248\n",
      "Epoch 73/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0221\n",
      "Epoch 74/100\n",
      "900/900 [==============================] - 0s 335us/step - loss: 0.0191\n",
      "Epoch 75/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0166\n",
      "Epoch 76/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0192\n",
      "Epoch 77/100\n",
      "900/900 [==============================] - 0s 323us/step - loss: 0.0231\n",
      "Epoch 78/100\n",
      "900/900 [==============================] - 0s 305us/step - loss: 0.0336\n",
      "Epoch 79/100\n",
      "900/900 [==============================] - 0s 316us/step - loss: 0.0292\n",
      "Epoch 80/100\n",
      "900/900 [==============================] - 0s 311us/step - loss: 0.0310\n",
      "Epoch 81/100\n",
      "900/900 [==============================] - 0s 307us/step - loss: 0.0229\n",
      "Epoch 82/100\n",
      "900/900 [==============================] - 0s 321us/step - loss: 0.0253\n",
      "Epoch 83/100\n",
      "900/900 [==============================] - 0s 316us/step - loss: 0.0264\n",
      "Epoch 84/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0287\n",
      "Epoch 85/100\n",
      "900/900 [==============================] - 0s 306us/step - loss: 0.0295\n",
      "Epoch 86/100\n",
      "900/900 [==============================] - 0s 320us/step - loss: 0.0265\n",
      "Epoch 87/100\n",
      "900/900 [==============================] - 0s 317us/step - loss: 0.0240\n",
      "Epoch 88/100\n",
      "900/900 [==============================] - 0s 308us/step - loss: 0.0236\n",
      "Epoch 89/100\n",
      "900/900 [==============================] - 0s 298us/step - loss: 0.0231\n",
      "Epoch 90/100\n",
      "900/900 [==============================] - 0s 338us/step - loss: 0.0212\n",
      "Epoch 91/100\n",
      "900/900 [==============================] - 0s 331us/step - loss: 0.0227\n",
      "Epoch 92/100\n",
      "900/900 [==============================] - 0s 287us/step - loss: 0.0195\n",
      "Epoch 93/100\n",
      "900/900 [==============================] - 0s 321us/step - loss: 0.0190\n",
      "Epoch 94/100\n",
      "900/900 [==============================] - 0s 314us/step - loss: 0.0176\n",
      "Epoch 95/100\n",
      "900/900 [==============================] - 0s 310us/step - loss: 0.0179\n",
      "Epoch 96/100\n",
      "900/900 [==============================] - 0s 322us/step - loss: 0.0211\n",
      "Epoch 97/100\n",
      "900/900 [==============================] - 0s 300us/step - loss: 0.0212\n",
      "Epoch 98/100\n",
      "900/900 [==============================] - 0s 299us/step - loss: 0.0197\n",
      "Epoch 99/100\n",
      "900/900 [==============================] - 0s 302us/step - loss: 0.0210\n",
      "Epoch 100/100\n",
      "900/900 [==============================] - 0s 301us/step - loss: 0.0272\n",
      "100/100 [==============================] - 3s 35ms/step\n",
      "Results: -1.52 (0.21) MSE\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "\n",
    "seed = 7\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "print(libIsing.shape)\n",
    "print(labelsIsing.shape)\n",
    "results = cross_val_score(estimator, libIsing, labelsIsing, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base model\n",
    "def modelDeep():    \n",
    "    #source: https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/?fbclid=IwAR3aemwOJ3jpoRH1mFUSMX9X2C5NLjVYTH_gi1jddEoYRgjcJGcEUj2IRy8\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(32, 32)))\n",
    "    model.add(tf.keras.layers.Dense(1024, input_dim=1, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(128, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "estimatorDeep = KerasRegressor(build_fn=modelDeep, epochs=20, batch_size=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -1.62 (0.20) MSE\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "resultsDeep = cross_val_score(estimatorDeep, libIsing, labelsIsing, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (resultsDeep.mean(), resultsDeep.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base model\n",
    "def modelWide():    \n",
    "    #source: https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/?fbclid=IwAR3aemwOJ3jpoRH1mFUSMX9X2C5NLjVYTH_gi1jddEoYRgjcJGcEUj2IRy8\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(32, 32)))\n",
    "    model.add(tf.keras.layers.Dense(4096, input_dim=1, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "estimatorWide = KerasRegressor(build_fn=modelWide, epochs=20, batch_size=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -6.03 (1.91) MSE\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "resultsWide = cross_val_score(estimatorWide, libIsing, labelsIsing, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (resultsWide.mean(), resultsWide.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the images at high and lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base complete\n",
      "deep complete\n",
      "wide complete\n"
     ]
    }
   ],
   "source": [
    "#Now to test the three of these on the images:\n",
    "\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=200, verbose=0)\n",
    "estimator.fit(libIsing,labelsIsing)\n",
    "print(\"base complete\")\n",
    "estimatorDeep = KerasRegressor(build_fn=modelDeep, epochs=20, batch_size=200, verbose=0)\n",
    "estimatorDeep.fit(libIsing,labelsIsing)\n",
    "print(\"deep complete\")\n",
    "estimatorWide = KerasRegressor(build_fn=modelWide, epochs=20, batch_size=200, verbose=0)\n",
    "estimatorWide.fit(libIsing,labelsIsing)\n",
    "print(\"wide complete\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1. -1. -1. ... -1. -1. -1.]\n",
      "  [-1. -1. -1. ... -1. -1. -1.]\n",
      "  [-1. -1. -1. ... -1. -1. -1.]\n",
      "  ...\n",
      "  [-1. -1. -1. ... -1. -1. -1.]\n",
      "  [-1. -1. -1. ... -1. -1. -1.]\n",
      "  [-1. -1. -1. ... -1. -1. -1.]]\n",
      "\n",
      " [[ 1.  1.  1. ...  1.  1.  1.]\n",
      "  [ 1.  1.  1. ...  1.  1.  1.]\n",
      "  [ 1.  1.  1. ... -1.  1.  1.]\n",
      "  ...\n",
      "  [-1.  1.  1. ... -1. -1. -1.]\n",
      "  [ 1.  1.  1. ...  1.  1.  1.]\n",
      "  [ 1.  1.  1. ...  1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1. -1. ...  1. -1. -1.]\n",
      "  [-1. -1. -1. ...  1. -1.  1.]\n",
      "  [ 1. -1. -1. ...  1.  1.  1.]\n",
      "  ...\n",
      "  [ 1. -1. -1. ...  1.  1. -1.]\n",
      "  [ 1.  1.  1. ...  1.  1.  1.]\n",
      "  [ 1.  1.  1. ...  1.  1. -1.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1. -1. -1. ... -1.  1.  1.]\n",
      "  [-1. -1. -1. ...  1. -1. -1.]\n",
      "  [ 1.  1. -1. ...  1. -1.  1.]\n",
      "  ...\n",
      "  [ 1.  1.  1. ... -1. -1. -1.]\n",
      "  [ 1. -1. -1. ...  1. -1. -1.]\n",
      "  [ 1. -1. -1. ... -1.  1.  1.]]\n",
      "\n",
      " [[-1. -1. -1. ... -1. -1. -1.]\n",
      "  [-1. -1. -1. ... -1. -1. -1.]\n",
      "  [-1. -1. -1. ... -1. -1. -1.]\n",
      "  ...\n",
      "  [-1. -1. -1. ... -1. -1. -1.]\n",
      "  [-1. -1. -1. ... -1. -1. -1.]\n",
      "  [-1. -1. -1. ... -1. -1. -1.]]\n",
      "\n",
      " [[ 1. -1.  1. ... -1.  1.  1.]\n",
      "  [-1. -1. -1. ...  1.  1.  1.]\n",
      "  [ 1. -1.  1. ... -1. -1.  1.]\n",
      "  ...\n",
      "  [-1. -1. -1. ... -1. -1.  1.]\n",
      "  [-1. -1. -1. ...  1. -1.  1.]\n",
      "  [-1.  1. -1. ...  1. -1. -1.]]]\n"
     ]
    }
   ],
   "source": [
    "print(test_img.reshape(200,32,32))\n",
    "\n",
    "\n",
    "wideGuess = estimator.predict(test_img.reshape(200,32,32))\n",
    "deepGuess = estimatorDeep.predict(test_img.reshape(200,32,32))\n",
    "baseGuess = estimatorWide.predict(test_img.reshape(200,32,32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x2160 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wideGuess = estimator.predict(test_img.reshape(200,32,32))\n",
    "deepGuess = estimatorDeep.predict(test_img.reshape(200,32,32))\n",
    "baseGuess = estimatorWide.predict(test_img.reshape(200,32,32))\n",
    "\n",
    "#plt.title(\"distribution of guesses\")\n",
    "fig, ax = plt.subplots(3,1,figsize= (10,30))\n",
    "ax[0].hist(wideGuess,40)\n",
    "ax[0].set_title(\"Wide Network Distribution\")\n",
    "ax[0].locator_params(axis='x', nbins=20)\n",
    "\n",
    "ax[1].hist(deepGuess,40)\n",
    "ax[1].set_title(\"Deep Network Distribution\")\n",
    "ax[1].locator_params(axis='x', nbins=20)\n",
    "ax[2].hist(baseGuess,40)\n",
    "ax[2].set_title(\"Base Network Distribution\")\n",
    "ax[2].locator_params(axis='x', nbins=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the deep and wide distributions performed better at separating the data into high and low temperatures. The low temperature appears to be around 1 K and the high at 4.5 K  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
